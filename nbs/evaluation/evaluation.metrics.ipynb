{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emY2VAEOQFd4"
      },
      "outputs": [],
      "source": [
        "# default_exp evaluation.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqkGlda-QFd8"
      },
      "source": [
        "# Metrics\n",
        "> Metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRJNsH7tQFd-"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrEi24lbQFd_"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKLax8lyQFeA"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def NDCG(true, pred):\n",
        "    match = pred.eq(true).nonzero(as_tuple=True)[1]\n",
        "    ncdg = torch.log(torch.Tensor([2])).div(torch.log(match + 2))\n",
        "    ncdg = ncdg.sum().div(pred.shape[0]).item()\n",
        "    return ncdg\n",
        "\n",
        "\n",
        "def APAK(true, pred):\n",
        "    k = pred.shape[1]\n",
        "    apak = pred.eq(true).div(torch.arange(k) + 1)\n",
        "    apak = apak.sum().div(pred.shape[0]).item()\n",
        "    return apak\n",
        "\n",
        "\n",
        "def HR(true, pred):\n",
        "    hr = pred.eq(true).sum().div(pred.shape[0]).item()\n",
        "    return hr\n",
        "\n",
        "\n",
        "def get_eval_metrics(scores, true, k=10):\n",
        "    test_items = [torch.LongTensor(list(item_scores.keys())) for item_scores in scores]\n",
        "    test_scores = [torch.Tensor(list(item_scores.values())) for item_scores in scores]\n",
        "    topk_indices = [s.topk(k).indices for s in test_scores]\n",
        "    topk_items = [item[idx] for item, idx in zip(test_items, topk_indices)]\n",
        "    pred = torch.vstack(topk_items)\n",
        "    ncdg = NDCG(true, pred)\n",
        "    apak = APAK(true, pred)\n",
        "    hr = HR(true, pred)\n",
        "\n",
        "    return ncdg, apak, hr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE2509W1QFeB",
        "outputId": "530a4a2a-5625-41e3-9501-e1b88ae8ad63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4261859357357025, 0.36666667461395264, 0.6000000238418579)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = [{1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
        "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
        "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
        "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
        "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1}]\n",
        "\n",
        "true = torch.tensor([[1],[1],[2],[3],[4]])\n",
        "metric = get_eval_metrics(scores, true, k=3)\n",
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGKj0VCjQFeD",
        "outputId": "140d7cac-9f2d-4a40-8f1e-24a7ec34aa22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0, 1.0)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# it should all 1, because all relevant items are in range k=3\n",
        "true = torch.tensor([[4],[4],[4],[4],[4]])\n",
        "metric = get_eval_metrics(scores, true, k=3)\n",
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Qr-sFEQFeF",
        "outputId": "ce1642a8-cf97-48a2-d0a0-370d34a5fc4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 0.0, 0.0)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# it should all 0, because no relevant item is in range k=3\n",
        "true = torch.tensor([[9],[1],[9],[1],[1]])\n",
        "metric = get_eval_metrics(scores, true, k=3)\n",
        "metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def get_eval_metrics_v2(pred_list, topk=10):\n",
        "    NDCG = 0.0\n",
        "    HIT = 0.0\n",
        "    MRR = 0.0\n",
        "    for rank in pred_list:\n",
        "        if rank < topk:\n",
        "            MRR += 1.0 / (rank + 1.0)\n",
        "            NDCG += 1.0 / np.log2(rank + 2.0)\n",
        "            HIT += 1.0\n",
        "    return HIT /len(pred_list), NDCG /len(pred_list), MRR /len(pred_list)"
      ],
      "metadata": {
        "id": "LXi1U8on-Zo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eq(np.round(get_eval_metrics_v2(pred_list = [1,3,2], topk=3), 2),\n",
        "        np.array([0.67, 0.38, 0.28]))\n",
        "test_eq(np.round(get_eval_metrics_v2(pred_list = [1,3,2], topk=2), 2),\n",
        "        np.array([0.33, 0.21, 0.17]))\n",
        "test_eq(np.round(get_eval_metrics_v2(pred_list = [0,0,0], topk=2), 2),\n",
        "        np.array([1., 1., 1.]))\n",
        "test_eq(np.round(get_eval_metrics_v2(pred_list = [3,3,3], topk=2), 2),\n",
        "        np.array([0., 0., 0.]))"
      ],
      "metadata": {
        "id": "tOv7D0gE-aXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def precision_at_k_per_sample(actual, predicted, topk):\n",
        "    num_hits = 0\n",
        "    for place in predicted:\n",
        "        if place in actual:\n",
        "            num_hits += 1\n",
        "    return num_hits / (topk + 0.0)"
      ],
      "metadata": {
        "id": "8ZIyb_qEApp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [0,1,4]\n",
        "actual = [0,1,2,3]\n",
        "test_eq(np.round(precision_at_k_per_sample(actual, predicted, topk=2), 2),\n",
        "        np.array([1.]))\n",
        "test_eq(np.round(precision_at_k_per_sample(actual, predicted, topk=3), 2),\n",
        "        np.array([0.67]))"
      ],
      "metadata": {
        "id": "yFT6ouybAy-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def precision_at_k(actual, predicted, topk):\n",
        "    sum_precision = 0.0\n",
        "    num_users = len(predicted)\n",
        "    for i in range(num_users):\n",
        "        act_set = set(actual[i])\n",
        "        pred_set = set(predicted[i][:topk])\n",
        "        sum_precision += len(act_set & pred_set) / float(topk)\n",
        "\n",
        "    return sum_precision / num_users"
      ],
      "metadata": {
        "id": "eLQQdn-QBNz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [[0,1,4], [1,3]]\n",
        "actual = [[0,1,2,3], [0,1,2]]\n",
        "test_eq(np.round(precision_at_k(actual, predicted, topk=2), 2),\n",
        "        np.array([0.75]))\n",
        "test_eq(np.round(precision_at_k(actual, predicted, topk=3), 2),\n",
        "        np.array([0.5]))"
      ],
      "metadata": {
        "id": "C20gWpjzBOeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def ap_at_k(actual, predicted, topk=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at topk.\n",
        "    This function computes the average precision at topk between two lists of\n",
        "    items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    topk : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at topk over the input lists\n",
        "    \"\"\"\n",
        "    if len(predicted)>topk:\n",
        "        predicted = predicted[:topk]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i,p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i+1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), topk)"
      ],
      "metadata": {
        "id": "k7V5_RWCFZPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [0,1,4]\n",
        "actual = [0,1,2,3]\n",
        "test_eq(np.round(ap_at_k(actual, predicted, topk=2), 2),\n",
        "        np.array([1.]))\n",
        "test_eq(np.round(ap_at_k(actual, predicted, topk=3), 2),\n",
        "        np.array([0.67]))"
      ],
      "metadata": {
        "id": "py46mbQDFfrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def map_at_k(actual, predicted, topk=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at topk.\n",
        "    This function computes the mean average prescision at topk between two lists\n",
        "    of lists of items.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    topk : int, optional\n",
        "        The maximum number of predicted elements\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at topk over the input lists\n",
        "    \"\"\"\n",
        "    return np.mean([ap_at_k(a, p, topk) for a, p in zip(actual, predicted)])"
      ],
      "metadata": {
        "id": "YB7WP8V6F94e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [[0,1,4], [1,3]]\n",
        "actual = [[0,1,2,3], [0,1,2]]\n",
        "test_eq(np.round(map_at_k(actual, predicted, topk=2), 2),\n",
        "        np.array([0.75]))\n",
        "test_eq(np.round(map_at_k(actual, predicted, topk=3), 2),\n",
        "        np.array([0.5]))"
      ],
      "metadata": {
        "id": "ewf57bdVGF3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def recall_at_k(actual, predicted, topk):\n",
        "    sum_recall = 0.0\n",
        "    num_users = len(predicted)\n",
        "    true_users = 0\n",
        "    recall_dict = {}\n",
        "    for i in range(num_users):\n",
        "        act_set = set(actual[i])\n",
        "        pred_set = set(predicted[i][:topk])\n",
        "        if len(act_set) != 0:\n",
        "            #sum_recall += len(act_set & pred_set) / float(len(act_set))\n",
        "            one_user_recall = len(act_set & pred_set) / float(len(act_set))\n",
        "            recall_dict[i] = one_user_recall\n",
        "            sum_recall += one_user_recall\n",
        "            true_users += 1\n",
        "    return sum_recall / true_users, recall_dict"
      ],
      "metadata": {
        "id": "qAPJ3txmBv-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [[0,1,4], [1,3]]\n",
        "actual = [[0,1,2,3], [0,1,2]]\n",
        "test_eq(np.round(recall_at_k(actual, predicted, topk=2)[0], 2),\n",
        "        np.array([0.42]))\n",
        "test_eq(np.round(recall_at_k(actual, predicted, topk=3)[0], 2),\n",
        "        np.array([0.42]))"
      ],
      "metadata": {
        "id": "al0SHUF5D6Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def cal_mrr(actual, predicted):\n",
        "    sum_mrr = 0.\n",
        "    true_users = 0\n",
        "    num_users = len(predicted)\n",
        "    mrr_dict = {}\n",
        "    for i in range(num_users):\n",
        "        r = []\n",
        "        act_set = set(actual[i])\n",
        "        pred_list = predicted[i]\n",
        "        for item in pred_list:\n",
        "            if item in act_set:\n",
        "                r.append(1)\n",
        "            else:\n",
        "                r.append(0)\n",
        "        r = np.array(r)\n",
        "        if np.sum(r) > 0:\n",
        "            #sum_mrr += np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n",
        "            one_user_mrr = np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n",
        "            sum_mrr += one_user_mrr\n",
        "            true_users += 1\n",
        "            mrr_dict[i] = one_user_mrr\n",
        "        else:\n",
        "            mrr_dict[i] = 0.\n",
        "    return sum_mrr / len(predicted), mrr_dict"
      ],
      "metadata": {
        "id": "iqTkMgQDEdEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [[0,1,4], [1,3]]\n",
        "actual = [[0,1], [0,1]]\n",
        "test_eq(np.round(cal_mrr(actual, predicted)[0], 2),\n",
        "        np.array([1.]))"
      ],
      "metadata": {
        "id": "NDgcplfDEgYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "def ndcg_at_k(actual, predicted, topk):\n",
        "    res = 0\n",
        "    ndcg_dict = {}\n",
        "    for user_id in range(len(actual)):\n",
        "        k = min(topk, len(actual[user_id]))\n",
        "        # idcg = idcg_at_k(k)\n",
        "        res = sum([1.0/math.log(i+2, 2) for i in range(k)])\n",
        "        idcg = res if res else 1.0\n",
        "        dcg_k = sum([int(predicted[user_id][j] in\n",
        "                         set(actual[user_id])) / math.log(j+2, 2) for j in range(topk)])\n",
        "        res += dcg_k / idcg\n",
        "        ndcg_dict[user_id] = dcg_k / idcg\n",
        "    return res / float(len(actual)), ndcg_dict"
      ],
      "metadata": {
        "id": "6SbaOTLJ-W1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [[0,1,4]]\n",
        "actual = [[0,1,2,3]]\n",
        "test_eq(np.round(ndcg_at_k(actual, predicted, topk=2)[0], 2),\n",
        "        np.array([2.63]))\n",
        "test_eq(np.round(ndcg_at_k(actual, predicted, topk=3)[0], 2),\n",
        "        np.array([2.9]))"
      ],
      "metadata": {
        "id": "hgouj_pxGPt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW-2rM2bQFeG"
      },
      "source": [
        "> **References:-**\n",
        "- https://github.com/massquantity/DBRL/blob/master/dbrl/evaluate/metrics.py\n",
        "- [https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/ranking_metric.py](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/ranking_metric.py)\n",
        "- [https://github.com/karlhigley/ranking-metrics-torch](https://github.com/karlhigley/ranking-metrics-torch)\n",
        "- [https://github.com/mquad/sars_tutorial/blob/master/util/metrics.py](https://github.com/mquad/sars_tutorial/blob/master/util/metrics.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLcMr29xQFeH",
        "outputId": "1884e228-4536-4d15-c5b1-5dca8cf41819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-06 09:02:26\n",
            "\n",
            "recohut: 0.0.9\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "torchmetrics: 0.6.2\n",
            "numpy       : 1.19.5\n",
            "torch       : 1.10.0+cu111\n",
            "PIL         : 7.1.2\n",
            "matplotlib  : 3.2.2\n",
            "IPython     : 5.5.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "evaluation.metrics.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}