{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AevOJuudui3b"
      },
      "outputs": [],
      "source": [
        "# default_exp models.widedeep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbsiSmziui3i"
      },
      "source": [
        "# WideDeep\n",
        "> A pytorch implementation of wide&deep model.\n",
        "\n",
        "Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img5.png)\n",
        "\n",
        "To understand the concept of deep & wide recommendations, it’s best to think of it as two separate, but collaborating, engines. The wide model, often referred to in the literature as the linear model, memorizes users and their past product choices. Its inputs may consist simply of a user identifier and a product identifier, though other attributes relevant to the pattern (such as time of day) may also be incorporated.\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img6.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img6.png)\n",
        "\n",
        "The deep portion of the model, so named as it is a deep neural network, examines the generalizable attributes of a user and their product choices. From these, the model learns the broader characteristics that tend to favor users’ product selections.\n",
        "\n",
        "Together, the wide and deep submodels are trained on historical product selections by individual users to predict future product selections. The end result is a single model capable of calculating the probability with which a user will purchase a given item, given both memorized past choices and generalizations about a user’s preferences. These probabilities form the basis for user-specific product rankings, which can be used for making recommendations.\n",
        "\n",
        "The goal with wide and deep recommenders is to provide the same level of customer intimacy that, for example, our favorite barista does. This model uses explicit and implicit feedback to expand the considerations set for customers. Wide and deep recommenders go beyond simple weighted averaging of customer feedback found in some collaborative filters to balance what is understood about the individual with what is known about similar customers. If done properly, the recommendations make the customer feel understood and this should translate into greater value for both the customer and the business.\n",
        "\n",
        "The intuitive logic of the wide-and-deep recommender belies the complexity of its actual construction. Inputs must be defined separately for each of the wide and deep portions of the model and each must be trained in a coordinated manner to arrive at a single output, but tuned using optimizers specific to the nature of each submodel. Thankfully, the **[Tensorflow DNNLinearCombinedClassifier estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)** provides a pre-packaged architecture, greatly simplifying the assembly of the overall model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L18eq4NCui3s"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v1"
      ],
      "metadata": {
        "id": "sI53n6FEu13L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from recohut.models.layers.embedding import EmbeddingLayer\n",
        "from recohut.models.layers.common import MLP_Layer, LR_Layer\n",
        "\n",
        "from recohut.models.bases.ctr import CTRModel"
      ],
      "metadata": {
        "id": "l9hT_TEfnMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class WideDeep(CTRModel):\n",
        "    def __init__(self, \n",
        "                 feature_map, \n",
        "                 model_id=\"WideDeep\",\n",
        "                 task=\"binary_classification\",\n",
        "                 learning_rate=1e-3, \n",
        "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
        "                 embedding_dim=10, \n",
        "                 hidden_units=[64, 64, 64], \n",
        "                 hidden_activations=\"ReLU\", \n",
        "                 net_dropout=0, \n",
        "                 batch_norm=False, \n",
        "                 **kwargs):\n",
        "        super(WideDeep, self).__init__(feature_map, \n",
        "                                           model_id=model_id,\n",
        "                                           **kwargs)\n",
        "        self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
        "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=False)\n",
        "        self.dnn = MLP_Layer(input_dim=embedding_dim * feature_map.num_fields,\n",
        "                             output_dim=1, \n",
        "                             hidden_units=hidden_units,\n",
        "                             hidden_activations=hidden_activations,\n",
        "                             output_activation=None, \n",
        "                             dropout_rates=net_dropout, \n",
        "                             batch_norm=batch_norm, \n",
        "                             use_bias=True)\n",
        "        self.output_activation = self.get_final_activation(task)\n",
        "        self.init_weights(embedding_initializer=embedding_initializer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature_emb = self.embedding_layer(inputs)\n",
        "        y_pred = self.lr_layer(inputs)\n",
        "        y_pred += self.dnn(feature_emb.flatten(start_dim=1))\n",
        "        if self.output_activation is not None:\n",
        "            y_pred = self.output_activation(y_pred)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "bWVxCkXgFpqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "ueSrnou3nj_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'model_id': 'WideDeep',\n",
        "              'data_dir': '/content/data',\n",
        "              'model_root': './checkpoints/',\n",
        "              'learning_rate': 1e-3,\n",
        "              'optimizer': 'adamw',\n",
        "              'task': 'binary_classification',\n",
        "              'loss': 'binary_crossentropy',\n",
        "              'metrics': ['logloss', 'AUC'],\n",
        "              'embedding_dim': 10,\n",
        "              'hidden_units': [300, 300, 300],\n",
        "              'hidden_activations': 'relu',\n",
        "              'net_regularizer': 0,\n",
        "              'embedding_regularizer': 0,\n",
        "              'batch_norm': False,\n",
        "              'net_dropout': 0,\n",
        "              'batch_size': 64,\n",
        "              'epochs': 3,\n",
        "              'shuffle': True,\n",
        "              'seed': 2019,\n",
        "              'use_hdf5': True,\n",
        "              'workers': 1,\n",
        "              'verbose': 0}"
      ],
      "metadata": {
        "id": "RF7l2_nfFpqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WideDeep(ds.dataset.feature_map, **params)"
      ],
      "metadata": {
        "id": "Xji9ukmkbsqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl_trainer(model, ds, max_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428,
          "referenced_widgets": [
            "55888d70355c4acf823631cda129a38c",
            "d239300f2d714a35b1c2362f64cbe3d6",
            "6ec8f6d292f04980bcb013bad3fe7002",
            "adeb03ec9acb4f86b1b43ed887e432f7",
            "05d81cc3df454cffbe72a18b8ccc7ce1",
            "b66743bfae9f4e16b0f4ba10f4564a22",
            "1acd4e0e5d954180bb1730cd786c7c77",
            "fbc074d4467a48b5a7b64143ad88606b",
            "96335595951b45d2a0afbb05cfad4c24",
            "29e7248ce3ed4bf5bfce06fe53601711",
            "56478a8695b6418eb754df0d8150f6a8",
            "beba2269f00a4aad9ea2702be7af5b7f",
            "d6db192d7ddc446a88178ed8fbe54f1e",
            "97d978d8f4a448b0b988ddc446a17d89",
            "c1f07e282d8e4deab97283117f21e6d6",
            "55af77edafa14571a6a97222c409d205",
            "5260aa9d8e7649d0b785fbec24757054",
            "3930fbabe240466bae8d4026bd85efd3",
            "68a171a7e680478396586f1ddd722430",
            "b13e51f823c348c7885921c86993176a",
            "f3bcbf368bb043b4aea52ba7f7e42831",
            "b8a62b7484834cd28a845f92f7b636a5",
            "e345f31a5ab541cd8ca3910c28e78232",
            "872d1a12294841a2ba8ab38ea7656b5b",
            "27a6763cfdb24a93a511c1004940bd71",
            "9023afac06034169b5727b246c2f4584",
            "23d4162a9fea453ea80948192fcc2e6b",
            "1f9586756f7546a79dd6dc16adf32383",
            "0366606af440499e9f70641ac199479f",
            "757375a671b6449aa0482cb772c7ff5e",
            "ed22c05ed52849db95869fd1702c1636",
            "28c8b3eff1494a49b8271b799b143a62",
            "41525818800d482d914a87377ea21b3d"
          ]
        },
        "outputId": "37cdfa1c-23d0-4273-9d16-e8fa6d031f80",
        "id": "bC4JxxC6bsqX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name              | Type           | Params\n",
            "-----------------------------------------------------\n",
            "0 | embedding_layer   | EmbeddingLayer | 4.8 K \n",
            "1 | lr_layer          | LR_Layer       | 476   \n",
            "2 | dnn               | MLP_Layer      | 223 K \n",
            "3 | output_activation | Sigmoid        | 0     \n",
            "-----------------------------------------------------\n",
            "228 K     Trainable params\n",
            "0         Non-trainable params\n",
            "228 K     Total params\n",
            "0.914     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55888d70355c4acf823631cda129a38c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beba2269f00a4aad9ea2702be7af5b7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e345f31a5ab541cd8ca3910c28e78232",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1533)}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1533)}}]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v2"
      ],
      "metadata": {
        "id": "Kyvaavo1uxSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY6rVXtRui3w"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "\n",
        "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear, MultiLayerPerceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNrHgXNdui3x"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class WideDeep_v2(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of wide and deep learning.\n",
        "    Reference:\n",
        "        HT Cheng, et al. Wide & Deep Learning for Recommender Systems, 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\n",
        "        super().__init__()\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
        "        self.embed_output_dim = len(field_dims) * embed_dim\n",
        "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        embed_x = self.embedding(x)\n",
        "        x = self.linear(x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
        "        return torch.sigmoid(x.squeeze(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sXgkxw0ui3y"
      },
      "source": [
        "> **References:-**\n",
        "- HT Cheng, et al. Wide & Deep Learning for Recommender Systems, 2016. https://arxiv.org/abs/1606.07792.\n",
        "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/wd.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFBZxIfYui30"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.wide_and_deep.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}