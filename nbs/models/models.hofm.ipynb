{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrWTvAkXydmL"
      },
      "outputs": [],
      "source": [
        "# default_exp models.hofm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgaOyP-TydmW"
      },
      "source": [
        "# HOFM\n",
        "> A pytorch implementation of Higher-Order Factorization Machines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r41XApXQydmc"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v1"
      ],
      "metadata": {
        "id": "gpRlXHD4ykxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from recohut.models.layers.embedding import EmbeddingLayer\n",
        "from recohut.models.layers.interaction import InnerProductLayer\n",
        "from recohut.models.layers.common import LR_Layer\n",
        "\n",
        "from recohut.models.bases.ctr import CTRModel"
      ],
      "metadata": {
        "id": "l9hT_TEfnMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class HOFM(CTRModel):\n",
        "    def __init__(self, \n",
        "                 feature_map, \n",
        "                 model_id=\"HOFM\",\n",
        "                 task=\"binary_classification\",\n",
        "                 learning_rate=1e-3, \n",
        "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
        "                 order=3,\n",
        "                 embedding_dim=10,\n",
        "                 reuse_embedding=False,\n",
        "                 **kwargs):\n",
        "        super(HOFM, self).__init__(feature_map, \n",
        "                                           model_id=model_id,\n",
        "                                           **kwargs)\n",
        "        self.order = order\n",
        "        assert order >= 2, \"order >= 2 is required in HOFM!\"\n",
        "        self.reuse_embedding = reuse_embedding\n",
        "        if reuse_embedding:\n",
        "            assert isinstance(embedding_dim, int), \"embedding_dim should be an integer when reuse_embedding=True.\"\n",
        "            self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
        "        else:\n",
        "            if not isinstance(embedding_dim, list):\n",
        "                embedding_dim = [embedding_dim] * (order - 1)\n",
        "            self.embedding_layers = nn.ModuleList([EmbeddingLayer(feature_map, embedding_dim[i]) \\\n",
        "                                                   for i in range(order - 1)])\n",
        "        self.inner_product_layer = InnerProductLayer(feature_map.num_fields)\n",
        "        self.lr_layer = LR_Layer(feature_map, use_bias=True)\n",
        "        self.output_activation = self.get_final_activation(task)\n",
        "        self.field_conjunction_dict = dict()\n",
        "        for order_i in range(3, self.order + 1):\n",
        "            order_i_conjunction = zip(*list(combinations(range(feature_map.num_fields), order_i)))\n",
        "            for k, field_index in enumerate(order_i_conjunction):\n",
        "                self.field_conjunction_dict[(order_i, k)] = torch.LongTensor(field_index)\n",
        "        self.init_weights(embedding_initializer=embedding_initializer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        y_pred = self.lr_layer(inputs)\n",
        "        if self.reuse_embedding:\n",
        "            feature_emb = self.embedding_layer(inputs)\n",
        "        for i in range(2, self.order + 1):\n",
        "            order_i_out = self.high_order_interaction(feature_emb if self.reuse_embedding \\\n",
        "                                                      else self.embedding_layers[i - 2](inputs), order_i=i)\n",
        "            y_pred += order_i_out\n",
        "        if self.output_activation is not None:\n",
        "            y_pred = self.output_activation(y_pred)\n",
        "        return y_pred\n",
        "\n",
        "    def high_order_interaction(self, feature_emb, order_i):\n",
        "        if order_i == 2:\n",
        "            interaction_out = self.inner_product_layer(feature_emb)\n",
        "        elif order_i > 2:\n",
        "            index = self.field_conjunction_dict[(order_i, 0)].to(self.device)\n",
        "            hadamard_product = torch.index_select(feature_emb, 1, index)\n",
        "            for k in range(1, order_i):\n",
        "                index = self.field_conjunction_dict[(order_i, k)].to(self.device)\n",
        "                hadamard_product = hadamard_product * torch.index_select(feature_emb, 1, index)\n",
        "            interaction_out = hadamard_product.sum((1, 2)).view(-1, 1)\n",
        "        return interaction_out"
      ],
      "metadata": {
        "id": "bWVxCkXgFpqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "ueSrnou3nj_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'model_id': 'HOFM',\n",
        "              'data_dir': '/content/data',\n",
        "              'model_root': './checkpoints/',\n",
        "              'learning_rate': 1e-3,\n",
        "              'optimizer': 'adamw',\n",
        "              'task': 'binary_classification',\n",
        "              'loss': 'binary_crossentropy',\n",
        "              'metrics': ['logloss', 'AUC'],\n",
        "              'regularizer': 0,\n",
        "              'order': 4,\n",
        "              'embedding_dim': 10,\n",
        "              'reuse_embedding': False,\n",
        "              'batch_size': 64,\n",
        "              'epochs': 3,\n",
        "              'shuffle': True,\n",
        "              'seed': 2019,\n",
        "              'use_hdf5': True,\n",
        "              'workers': 1,\n",
        "              'verbose': 0}"
      ],
      "metadata": {
        "id": "RF7l2_nfFpqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HOFM(ds.dataset.feature_map, **params)"
      ],
      "metadata": {
        "id": "JCSydm-7fMLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl_trainer(model, ds, max_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428,
          "referenced_widgets": [
            "a0314fac5b1547d593f8a1929d560359",
            "2b907cff4ef340afb3ad0a46d9b98941",
            "78744d32eacd4dea902958de953b2a94",
            "f6525a038e72436b908dda96e2c9f7a9",
            "9442708cd86341a48c0bad478d68ad5a",
            "9edcd465047044469db46c0199aa80fc",
            "b1d6879d1796456b88bdc8654afc6d90",
            "94b49e9177b9475283bcf6b55ba65c0a",
            "4d4acec5e904418a89b164bc2f172355",
            "9f5dc7aa9d574ab8b6bb90bbbf1b05a3",
            "f77f6c037d1a4871848ce169493ca808",
            "386fc95a2ae9440eaa453221df66f7be",
            "5a04eb06718c440f87940881ea1798e6",
            "4fa474665f4d40919f2f67cd3a6204c6",
            "b9fa5af4daf84f08ac3eb30f20be1522",
            "0161679ecf3f48c8ae88ba8276d8a8ef",
            "be60915e84fd4f61b3b3df897e47c404",
            "78c0bd3f20d843279c16753eba367763",
            "5bf4b939413643c6a6d0684020b40858",
            "b99cdc61fab445d3a53d6c2cb1486b91",
            "aab5338b6be0484cb715321ee495a50c",
            "af0d088a882f410c9b060eae093dfac8",
            "b3af172c4631482ca8ae164deb3a2c32",
            "8207b132298048949363c06e3a0e1b71",
            "89d166fe73284990ae03b3c3bb5b38dd",
            "012697406c5d47f5a5c24fe1cc72d500",
            "9cc35aa6b84c4e46b7c0650adcbc300f",
            "c8c2f5ac763e43fd9d9645de213c8ce6",
            "09b87d75c3f84c69aefc4373d880d458",
            "6737f90b72b145ecbf2a1fd0528544c1",
            "103ce91823ee422cad9a390fea18de6d",
            "78b46fc684bb4b3aa9fe14fe4c334ec8",
            "537c8b85a4db4b16816056a3f02d5eed"
          ]
        },
        "outputId": "f532c1af-0169-4709-e0a2-1e3e19022d64",
        "id": "fJ7fgeh8fMLJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name                | Type              | Params\n",
            "----------------------------------------------------------\n",
            "0 | embedding_layers    | ModuleList        | 14.3 K\n",
            "1 | inner_product_layer | InnerProductLayer | 378   \n",
            "2 | lr_layer            | LR_Layer          | 477   \n",
            "3 | output_activation   | Sigmoid           | 0     \n",
            "----------------------------------------------------------\n",
            "14.8 K    Trainable params\n",
            "378       Non-trainable params\n",
            "15.1 K    Total params\n",
            "0.061     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0314fac5b1547d593f8a1929d560359",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "386fc95a2ae9440eaa453221df66f7be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3af172c4631482ca8ae164deb3a2c32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5169)}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5169)}}]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v2"
      ],
      "metadata": {
        "id": "lbRHC_ewygcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXKoo5KIydmg"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "\n",
        "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itQE4qumydmi"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class FactorizationMachine(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, reduce_sum=True):\n",
        "        super().__init__()\n",
        "        self.reduce_sum = reduce_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
        "        \"\"\"\n",
        "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
        "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
        "        ix = square_of_sum - sum_of_square\n",
        "        if self.reduce_sum:\n",
        "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
        "        return 0.5 * ix\n",
        "\n",
        "class AnovaKernel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, order, reduce_sum=True):\n",
        "        super().__init__()\n",
        "        self.order = order\n",
        "        self.reduce_sum = reduce_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
        "        \"\"\"\n",
        "        batch_size, num_fields, embed_dim = x.shape\n",
        "        a_prev = torch.ones((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
        "        for t in range(self.order):\n",
        "            a = torch.zeros((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
        "            a[:, t+1:, :] += x[:, t:, :] * a_prev[:, t:-1, :]\n",
        "            a = torch.cumsum(a, dim=1)\n",
        "            a_prev = a\n",
        "        if self.reduce_sum:\n",
        "            return torch.sum(a[:, -1, :], dim=-1, keepdim=True)\n",
        "        else:\n",
        "            return a[:, -1, :]\n",
        "\n",
        "class HOFM_v2(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Higher-Order Factorization Machines.\n",
        "    Reference:\n",
        "        M Blondel, et al. Higher-Order Factorization Machines, 2016.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, order, embed_dim):\n",
        "        super().__init__()\n",
        "        if order < 1:\n",
        "            raise ValueError(f'invalid order: {order}')\n",
        "        self.order = order\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        if order >= 2:\n",
        "            self.embedding = FeaturesEmbedding(field_dims, embed_dim * (order - 1))\n",
        "            self.fm = FactorizationMachine(reduce_sum=True)\n",
        "        if order >= 3:\n",
        "            self.kernels = torch.nn.ModuleList([\n",
        "                AnovaKernel(order=i, reduce_sum=True) for i in range(3, order + 1)\n",
        "            ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        y = self.linear(x).squeeze(1)\n",
        "        if self.order >= 2:\n",
        "            x = self.embedding(x)\n",
        "            x_part = x[:, :, :self.embed_dim]\n",
        "            y += self.fm(x_part).squeeze(1)\n",
        "            for i in range(self.order - 2):\n",
        "                x_part = x[:, :, (i + 1) * self.embed_dim: (i + 2) * self.embed_dim]\n",
        "                y += self.kernels[i](x_part).squeeze(1)\n",
        "        return torch.sigmoid(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWNDlAaHydmn"
      },
      "source": [
        "> **References:-**\n",
        "- M Blondel, et al. Higher-Order Factorization Machines, 2016.\n",
        "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/hofm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz0by4dLydmp"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.hofm.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}