{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dcOyjiap9O_"
      },
      "outputs": [],
      "source": [
        "# default_exp models.autoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPG8shT2p9PF"
      },
      "source": [
        "# AutoInt\n",
        "> A pytorch implementation of AutoInt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THsVpHHzp9PH"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v1"
      ],
      "metadata": {
        "id": "8ZdkADtqqDZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from recohut.models.layers.embedding import EmbeddingLayer\n",
        "from recohut.models.layers.common import MLP_Layer, LR_Layer\n",
        "from recohut.models.layers.attention import MultiHeadSelfAttention\n",
        "\n",
        "from recohut.models.bases.ctr import CTRModel"
      ],
      "metadata": {
        "id": "l9hT_TEfnMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class AutoInt(CTRModel):\n",
        "    def __init__(self, \n",
        "                 feature_map, \n",
        "                 model_id=\"AutoInt\",\n",
        "                 task=\"binary_classification\",\n",
        "                 learning_rate=1e-3, \n",
        "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
        "                 embedding_dim=10, \n",
        "                 dnn_hidden_units=[64, 64, 64], \n",
        "                 dnn_activations=\"ReLU\", \n",
        "                 attention_layers=2,\n",
        "                 num_heads=1,\n",
        "                 attention_dim=8,\n",
        "                 net_dropout=0, \n",
        "                 batch_norm=False,\n",
        "                 layer_norm=False,\n",
        "                 use_scale=False,\n",
        "                 use_wide=False,\n",
        "                 use_residual=True,\n",
        "                 **kwargs):\n",
        "        super(AutoInt, self).__init__(feature_map, \n",
        "                                           model_id=model_id,\n",
        "                                           **kwargs)\n",
        "        self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
        "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=False) \\\n",
        "                        if use_wide else None\n",
        "        self.dnn = MLP_Layer(input_dim=embedding_dim * feature_map.num_fields,\n",
        "                             output_dim=1, \n",
        "                             hidden_units=dnn_hidden_units,\n",
        "                             hidden_activations=dnn_activations,\n",
        "                             output_activation=None, \n",
        "                             dropout_rates=net_dropout, \n",
        "                             batch_norm=batch_norm, \n",
        "                             use_bias=True) \\\n",
        "                   if dnn_hidden_units else None # in case no DNN used\n",
        "        self.self_attention = nn.Sequential(\n",
        "            *[MultiHeadSelfAttention(embedding_dim if i == 0 else num_heads * attention_dim,\n",
        "                                    attention_dim=attention_dim, \n",
        "                                    num_heads=num_heads, \n",
        "                                    dropout_rate=net_dropout, \n",
        "                                    use_residual=use_residual, \n",
        "                                    use_scale=use_scale, \n",
        "                                    layer_norm=layer_norm,\n",
        "                                    align_to=\"output\") \n",
        "             for i in range(attention_layers)])\n",
        "        self.fc = nn.Linear(feature_map.num_fields * attention_dim * num_heads, 1)\n",
        "        self.output_activation = self.get_final_activation(task)\n",
        "        self.init_weights(embedding_initializer=embedding_initializer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature_emb = self.embedding_layer(inputs)\n",
        "        attention_out = self.self_attention(feature_emb)\n",
        "        attention_out = torch.flatten(attention_out, start_dim=1)\n",
        "        y_pred = self.fc(attention_out)\n",
        "        if self.dnn is not None:\n",
        "            y_pred += self.dnn(feature_emb.flatten(start_dim=1))\n",
        "        if self.lr_layer is not None:\n",
        "            y_pred += self.lr_layer(X)\n",
        "        if self.output_activation is not None:\n",
        "            y_pred = self.output_activation(y_pred)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "bWVxCkXgFpqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "ueSrnou3nj_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'model_id': 'AutoInt',\n",
        "              'data_dir': '/content/data',\n",
        "              'model_root': './checkpoints/',\n",
        "              'learning_rate': 1e-3,\n",
        "              'optimizer': 'adamw',\n",
        "              'task': 'binary_classification',\n",
        "              'loss': 'binary_crossentropy',\n",
        "              'metrics': ['logloss', 'AUC'],\n",
        "              'embedding_dim': 10,\n",
        "              'dnn_hidden_units': [400, 400],\n",
        "              'dnn_activations': 'relu',\n",
        "              'net_dropout': 0,\n",
        "              'num_heads': 2,\n",
        "              'attention_layers': 3,\n",
        "              'attention_dim': 40,\n",
        "              'use_residual': True,\n",
        "              'batch_norm': False,\n",
        "              'layer_norm': False,\n",
        "              'use_scale': False,\n",
        "              'use_wide': False,\n",
        "              'batch_size': 64,\n",
        "              'epochs': 3,\n",
        "              'shuffle': True,\n",
        "              'seed': 2019,\n",
        "              'use_hdf5': True,\n",
        "              'workers': 1,\n",
        "              'verbose': 0}"
      ],
      "metadata": {
        "id": "RF7l2_nfFpqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoInt(ds.dataset.feature_map, **params)"
      ],
      "metadata": {
        "id": "xB_83TdTHxpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl_trainer(model, ds, max_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "e4c2144a82de47aab2d2139e026451c0",
            "d6767e7231c348cba56cd71e3ac169da",
            "9a2398b3045f4ef3af48b37fb75381e3",
            "9558d7ef67f5467d97efe639fe741c70",
            "cea070a89eef41208ce8edf9fd2aea28",
            "bb0b804a0aaf430481c45458c7485a3d",
            "0e26ec036aa5438b96ba45a18eac7a8d",
            "5bbe07ea23dc417d9e9fa58d58c4c532",
            "a5acfe87fe184ddb9071019ca5abe010",
            "4936473f63774e759feace8028cf4d18",
            "57dcb62485494dc1b242ac7bc8446849",
            "f04011affc4f49a395a1200a7ca3aed0",
            "f80a2b38eb894967bf286e0c345f47c6",
            "fe30013c0d144201a4f7c83b965f0916",
            "b339cf837d604288b520a1f0894fc948",
            "3da28d3a4f464b6988998c28c89c6551",
            "502d854cc9fa4e729f5c9f1668e4d918",
            "5273866d681f4355b2fcb316d216db10",
            "bbc076ed589c4c84a21e740371fa1a3d",
            "d59c524e7e55448581badfd6d180d7b6",
            "638c532bb52048f3b7e824c5ad1cc71e",
            "35c1d94189124e0f98ecadbd7753e817",
            "7664a5cf645948e6b2ffbc03ea941b4e",
            "9cf2bb2eb3c5487dbc6e2a560efe9acb",
            "cf509b05d072483cbc022962ee8dcc57",
            "6c9cdefd8d9244388248de4a70f06cbb",
            "77192e02fd8a41a8a84ab52b76347798",
            "adc32851ea6242bc8a04804f71d121cd",
            "e5ea5d64602d430aa68cf3c1666ebc9e",
            "305865aa787d44279cc101905776e05b",
            "f08a6ba880f046ed9561b8d3803a09a1",
            "34d12f3301fc4b6599759672da3197d1",
            "35ea43caae584b1ba2936183cf02963e"
          ]
        },
        "outputId": "4e5321a9-ac43-4cbd-ecda-049a8573093d",
        "id": "PIaY4gpjHxpx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name              | Type           | Params\n",
            "-----------------------------------------------------\n",
            "0 | embedding_layer   | EmbeddingLayer | 4.8 K \n",
            "1 | dnn               | MLP_Layer      | 217 K \n",
            "2 | self_attention    | Sequential     | 41.6 K\n",
            "3 | fc                | Linear         | 1.1 K \n",
            "4 | output_activation | Sigmoid        | 0     \n",
            "-----------------------------------------------------\n",
            "264 K     Trainable params\n",
            "0         Non-trainable params\n",
            "264 K     Total params\n",
            "1.059     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4c2144a82de47aab2d2139e026451c0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f04011affc4f49a395a1200a7ca3aed0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7664a5cf645948e6b2ffbc03ea941b4e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1913)}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1913)}}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v2"
      ],
      "metadata": {
        "id": "3TOV3tHyp-Sq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A0seRhzp9PI"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear, MultiLayerPerceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clG-18XZp9PJ"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class AutoInt_v2(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of AutoInt.\n",
        "    Reference:\n",
        "        W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True):\n",
        "        super().__init__()\n",
        "        self.num_fields = len(field_dims)\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
        "        self.atten_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
        "        self.embed_output_dim = len(field_dims) * embed_dim\n",
        "        self.atten_output_dim = len(field_dims) * atten_embed_dim\n",
        "        self.has_residual = has_residual\n",
        "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropouts[1])\n",
        "        self.self_attns = torch.nn.ModuleList([\n",
        "            torch.nn.MultiheadAttention(atten_embed_dim, num_heads, dropout=dropouts[0]) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.attn_fc = torch.nn.Linear(self.atten_output_dim, 1)\n",
        "        if self.has_residual:\n",
        "            self.V_res_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        embed_x = self.embedding(x)\n",
        "        atten_x = self.atten_embedding(embed_x)\n",
        "        cross_term = atten_x.transpose(0, 1)\n",
        "        for self_attn in self.self_attns:\n",
        "            cross_term, _ = self_attn(cross_term, cross_term, cross_term)\n",
        "        cross_term = cross_term.transpose(0, 1)\n",
        "        if self.has_residual:\n",
        "            V_res = self.V_res_embedding(embed_x)\n",
        "            cross_term += V_res\n",
        "        cross_term = F.relu(cross_term).contiguous().view(-1, self.atten_output_dim)\n",
        "        x = self.linear(x) + self.attn_fc(cross_term) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
        "        return torch.sigmoid(x.squeeze(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_lMcOKQp9PL"
      },
      "source": [
        "> **References:-**\n",
        "- W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018. https://arxiv.org/abs/1810.11921.\n",
        "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/afi.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV4crkNhp9PM"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.autoint.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}