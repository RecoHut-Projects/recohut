{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M354921 | models > afm",
      "provenance": [],
      "collapsed_sections": [
        "1KypvcFZI64_"
      ],
      "toc_visible": true,
      "mount_file_id": "1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a",
      "authorship_tag": "ABX9TyNoU6OsHhEAzjzyoVXuKkQc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fycOO2OxKHEF"
      },
      "outputs": [],
      "source": [
        "# default_exp models.afm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KypvcFZI64_"
      },
      "source": [
        "# AFM\n",
        "> A pytorch implementation of Attentional Factorization Machines (AFM).\n",
        "\n",
        "Improves FM by discriminating the importance of different feature interactions. It learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img11.png)\n",
        "\n",
        "Formally, the AFM model can be defined as:\n",
        "\n",
        "$$\\hat{y}_{AFM} (x) = w_0 + \\sum_{i=1}^nw_ix_i + p^T\\sum_{i=1}^n\\sum_{j=i+1}^na_{ij}(v_i\\odot v_j)x_ix_j$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwuVx5oI65E"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ibt_XlI65I"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "\n",
        "from recohut.layers.common import FeaturesEmbedding, FeaturesLinear, MultiLayerPerceptron"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class AttentionalFactorizationMachine(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, attn_size, dropouts):\n",
        "        super().__init__()\n",
        "        self.attention = torch.nn.Linear(embed_dim, attn_size)\n",
        "        self.projection = torch.nn.Linear(attn_size, 1)\n",
        "        self.fc = torch.nn.Linear(embed_dim, 1)\n",
        "        self.dropouts = dropouts\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
        "        \"\"\"\n",
        "        num_fields = x.shape[1]\n",
        "        row, col = list(), list()\n",
        "        for i in range(num_fields - 1):\n",
        "            for j in range(i + 1, num_fields):\n",
        "                row.append(i), col.append(j)\n",
        "        p, q = x[:, row], x[:, col]\n",
        "        inner_product = p * q\n",
        "        attn_scores = F.relu(self.attention(inner_product))\n",
        "        attn_scores = F.softmax(self.projection(attn_scores), dim=1)\n",
        "        attn_scores = F.dropout(attn_scores, p=self.dropouts[0], training=self.training)\n",
        "        attn_output = torch.sum(attn_scores * inner_product, dim=1)\n",
        "        attn_output = F.dropout(attn_output, p=self.dropouts[1], training=self.training)\n",
        "        return self.fc(attn_output)\n",
        "\n",
        "\n",
        "class AFM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Attentional Factorization Machine.\n",
        "    Reference:\n",
        "        J Xiao, et al. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, 2017.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim, attn_size, dropouts):\n",
        "        super().__init__()\n",
        "        self.num_fields = len(field_dims)\n",
        "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.afm = AttentionalFactorizationMachine(embed_dim, attn_size, dropouts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = self.linear(x) + self.afm(self.embedding(x))\n",
        "        return torch.sigmoid(x.squeeze(1))"
      ],
      "metadata": {
        "id": "DchSUXOEK3CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **References:-**\n",
        "- J Xiao, et al. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, 2017.\n",
        "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/afm.py"
      ],
      "metadata": {
        "id": "VmBYJuNFiO3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXwRDjpKI65c"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ]
}