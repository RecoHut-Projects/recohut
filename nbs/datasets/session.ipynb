{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M880810 | datasets > session",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a",
      "authorship_tag": "ABX9TyOjJdiOK87clm9aZAcj6rmU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fycOO2OxKHEF"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KypvcFZI64_"
      },
      "source": [
        "# Sample Session dataset\n",
        "> Small sample of session dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwuVx5oI65E"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ibt_XlI65I"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from typing import List, Optional, Callable, Union, Any, Tuple\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "from collections.abc import Sequence\n",
        "import sys\n",
        "import csv\n",
        "import pickle\n",
        "import math\n",
        "import operator\n",
        "\n",
        "import numpy as np\n",
        "from datetime import timezone, datetime, timedelta\n",
        "import time\n",
        "\n",
        "from recohut.datasets.base import Dataset\n",
        "from recohut.utils.common_utils import download_url, makedirs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class SampleSessionDataset(Dataset):\n",
        "    url = 'https://github.com/RecoHut-Datasets/sample_session/raw/v1/sample_train-item-views.csv'\n",
        "\n",
        "    def __init__(self, root):\n",
        "        super().__init__(root)\n",
        "        \n",
        "        self._process()\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        return 'sample_train-item-views.csv'\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        return ['train.txt','test.txt','all_train_seq.txt']\n",
        "\n",
        "    def download(self):\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "\n",
        "    def process(self):\n",
        "        with open(self.raw_paths[0], \"r\") as f:\n",
        "            reader = csv.DictReader(f, delimiter=';')\n",
        "            sess_clicks = {}\n",
        "            sess_date = {}\n",
        "            ctr = 0\n",
        "            curid = -1\n",
        "            curdate = None\n",
        "            for data in reader:\n",
        "                sessid = data['session_id']\n",
        "                if curdate and not curid == sessid:\n",
        "                    date = ''\n",
        "                    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "                    sess_date[curid] = date\n",
        "                curid = sessid\n",
        "                item = data['item_id'], int(data['timeframe'])\n",
        "                curdate = ''\n",
        "                curdate = data['eventdate']\n",
        "\n",
        "                if sessid in sess_clicks:\n",
        "                    sess_clicks[sessid] += [item]\n",
        "                else:\n",
        "                    sess_clicks[sessid] = [item]\n",
        "                ctr += 1\n",
        "            date = ''\n",
        "            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "            for i in list(sess_clicks):\n",
        "                sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n",
        "                sess_clicks[i] = [c[0] for c in sorted_clicks]\n",
        "            sess_date[curid] = date\n",
        "\n",
        "        print(\"-- Reading data\")\n",
        "\n",
        "        # Filter out length 1 sessions\n",
        "        for s in list(sess_clicks):\n",
        "            if len(sess_clicks[s]) == 1:\n",
        "                del sess_clicks[s]\n",
        "                del sess_date[s]\n",
        "\n",
        "        # Count number of times each item appears\n",
        "        iid_counts = {}\n",
        "        for s in sess_clicks:\n",
        "            seq = sess_clicks[s]\n",
        "            for iid in seq:\n",
        "                if iid in iid_counts:\n",
        "                    iid_counts[iid] += 1\n",
        "                else:\n",
        "                    iid_counts[iid] = 1\n",
        "\n",
        "        sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n",
        "\n",
        "        length = len(sess_clicks)\n",
        "        for s in list(sess_clicks):\n",
        "            curseq = sess_clicks[s]\n",
        "            filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n",
        "            if len(filseq) < 2:\n",
        "                del sess_clicks[s]\n",
        "                del sess_date[s]\n",
        "            else:\n",
        "                sess_clicks[s] = filseq\n",
        "\n",
        "        # Split out test set based on dates\n",
        "        dates = list(sess_date.items())\n",
        "        maxdate = dates[0][1]\n",
        "\n",
        "        for _, date in dates:\n",
        "            if maxdate < date:\n",
        "                maxdate = date\n",
        "\n",
        "        # 7 days for test\n",
        "        splitdate = 0\n",
        "        splitdate = maxdate - 86400 * 7\n",
        "\n",
        "        print('Splitting date', splitdate)\n",
        "        tra_sess = filter(lambda x: x[1] < splitdate, dates)\n",
        "        tes_sess = filter(lambda x: x[1] > splitdate, dates)\n",
        "\n",
        "        # Sort sessions by date\n",
        "        tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "        tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "\n",
        "        print(len(tra_sess))\n",
        "        print(len(tes_sess))\n",
        "        print(tra_sess[:3])\n",
        "        print(tes_sess[:3])\n",
        "\n",
        "        print(\"-- Splitting train set and test set\")\n",
        "\n",
        "        item_dict = {}\n",
        "        # Convert training sessions to sequences and renumber items to start from 1\n",
        "        def obtian_tra():\n",
        "            train_ids = []\n",
        "            train_seqs = []\n",
        "            train_dates = []\n",
        "            item_ctr = 1\n",
        "            for s, date in tra_sess:\n",
        "                seq = sess_clicks[s]\n",
        "                outseq = []\n",
        "                for i in seq:\n",
        "                    if i in item_dict:\n",
        "                        outseq += [item_dict[i]]\n",
        "                    else:\n",
        "                        outseq += [item_ctr]\n",
        "                        item_dict[i] = item_ctr\n",
        "                        item_ctr += 1\n",
        "                if len(outseq) < 2:  # Doesn't occur\n",
        "                    continue\n",
        "                train_ids += [s]\n",
        "                train_dates += [date]\n",
        "                train_seqs += [outseq]\n",
        "            print(item_ctr)     # 43098, 37484\n",
        "            return train_ids, train_dates, train_seqs\n",
        "\n",
        "        # Convert test sessions to sequences, ignoring items that do not appear in training set\n",
        "        def obtian_tes():\n",
        "            test_ids = []\n",
        "            test_seqs = []\n",
        "            test_dates = []\n",
        "            for s, date in tes_sess:\n",
        "                seq = sess_clicks[s]\n",
        "                outseq = []\n",
        "                for i in seq:\n",
        "                    if i in item_dict:\n",
        "                        outseq += [item_dict[i]]\n",
        "                if len(outseq) < 2:\n",
        "                    continue\n",
        "                test_ids += [s]\n",
        "                test_dates += [date]\n",
        "                test_seqs += [outseq]\n",
        "            return test_ids, test_dates, test_seqs\n",
        "\n",
        "        tra_ids, tra_dates, tra_seqs = obtian_tra()\n",
        "        tes_ids, tes_dates, tes_seqs = obtian_tes()\n",
        "\n",
        "        def process_seqs(iseqs, idates):\n",
        "            out_seqs = []\n",
        "            out_dates = []\n",
        "            labs = []\n",
        "            ids = []\n",
        "            for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n",
        "                for i in range(1, len(seq)):\n",
        "                    tar = seq[-i]\n",
        "                    labs += [tar]\n",
        "                    out_seqs += [seq[:-i]]\n",
        "                    out_dates += [date]\n",
        "                    ids += [id]\n",
        "            return out_seqs, out_dates, labs, ids\n",
        "\n",
        "        tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n",
        "        te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n",
        "        tra = (tr_seqs, tr_labs)\n",
        "        tes = (te_seqs, te_labs)\n",
        "\n",
        "        print(len(tr_seqs))\n",
        "        print(len(te_seqs))\n",
        "        print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n",
        "        print(te_seqs[:3], te_dates[:3], te_labs[:3])\n",
        "\n",
        "        all = 0\n",
        "\n",
        "        for seq in tra_seqs:\n",
        "            all += len(seq)\n",
        "        for seq in tes_seqs:\n",
        "            all += len(seq)\n",
        "        print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
        "\n",
        "        pickle.dump(tra, open(self.processed_paths[0], 'wb'))\n",
        "        pickle.dump(tes, open(self.processed_paths[1], 'wb'))\n",
        "        pickle.dump(tra_seqs, open(self.processed_paths[2], 'wb'))\n",
        "\n",
        "        print('Done.')"
      ],
      "metadata": {
        "id": "gVlezK7_tIO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = SampleSessionDataset(root='/content/sample')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbqzJvo6wIjt",
        "outputId": "18da4cfb-9e0a-4502-c688-706c4c286e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Reading data\n",
            "Splitting date 1464134400.0\n",
            "469\n",
            "47\n",
            "[('2671', 1451952000.0), ('1211', 1452384000.0), ('3780', 1452384000.0)]\n",
            "[('1864', 1464220800.0), ('1867', 1464220800.0), ('1868', 1464220800.0)]\n",
            "-- Splitting train set and test set\n",
            "310\n",
            "1205\n",
            "99\n",
            "[[1, 2], [1], [4]] [1451952000.0, 1451952000.0, 1452384000.0] [3, 2, 5]\n",
            "[[282], [281, 308], [281]] [1464220800.0, 1464220800.0, 1464220800.0] [282, 281, 308]\n",
            "avg length:  3.5669291338582676\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "def data_masks(all_usr_pois, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    return us_pois, us_msks, len_max"
      ],
      "metadata": {
        "id": "zk_IpCywcWt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "def split_validation(train_set, valid_portion):\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)"
      ],
      "metadata": {
        "id": "eH6FBtDscbBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class GraphData():\n",
        "    def __init__(self, data, shuffle=False, graph=None):\n",
        "        inputs = data[0]\n",
        "        inputs, mask, len_max = data_masks(inputs, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "        self.graph = graph\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, i):\n",
        "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
        "        items, n_node, A, alias_inputs = [], [], [], []\n",
        "        for u_input in inputs:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in inputs:\n",
        "            node = np.unique(u_input)\n",
        "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
        "            A.append(u_A)\n",
        "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "        return alias_inputs, A, items, mask, targets"
      ],
      "metadata": {
        "id": "eZs8gRnqca_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pickle.load(open('/content/sample/processed/train.txt', 'rb'))\n",
        "train_data[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5bY09g-ca8z",
        "outputId": "d46c01be-6dd3-4714-dc29-7d73c848bbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2], [1], [4], [6], [8, 9], [8], [10, 11, 11], [10, 11], [10], [12]]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUj8yhlmeRGt",
        "outputId": "f96d160e-121a-4505-c2f9-bec9c22aa69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1205"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data = split_validation(train_data, valid_portion=0.1)\n",
        "test_data = valid_data\n",
        "\n",
        "train_data = GraphData(train_data, shuffle=True)\n",
        "test_data = GraphData(test_data, shuffle=False)"
      ],
      "metadata": {
        "id": "VB8Qcl2yeL6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.generate_batch(10)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OKoc2qud3Sy",
        "outputId": "1a0c2a0b-1167-484b-cfa4-8528770b6e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
              " array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
              " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
              " array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXwRDjpKI65c",
        "outputId": "c9285840-7325-4fe3-bee9-f557af1ceaea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2021-12-22 18:50:17\n",
            "\n",
            "recohut: 0.0.6\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "csv    : 1.0\n",
            "sys    : 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "IPython: 5.5.0\n",
            "numpy  : 1.19.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ]
}