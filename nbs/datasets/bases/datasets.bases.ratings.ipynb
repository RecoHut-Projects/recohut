{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms1RfbNVeRIb"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.bases.ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAzceUi3eRIk"
      },
      "source": [
        "# Ratings dataset\n",
        "> Base class for ratings dataset module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgdIIOUZeRIq"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aDNCoBdAeRIs"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from typing import List, Optional, Callable, Union, Any, Tuple\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "from recohut.utils.common_utils import download_url, extract_zip, extract_gz, makedirs\n",
        "from recohut.datasets.bases.common import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9CTySmXyeRJZ"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class RatingDataset(Dataset):\n",
        "    r\"\"\"Interaction data with rating feedback\n",
        "    \n",
        "    Args:\n",
        "        root: data folder path\n",
        "        min_uc: minimum user count to keep in the data\n",
        "        min_sc: minimum item count to keep in the data\n",
        "        split: data split method - leave_one_out/holdout\n",
        "        min_rating: minimum rating threshold to convert explicit feedback into implicit\n",
        "\n",
        "    References:\n",
        "        1. https://github.com/Yueeeeeeee/RecSys-Extraction-Attack/tree/main/datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, root, min_uc, min_sc, split='leave_one_out', dataset_split_seed=42,\n",
        "                 eval_set_size=None, min_rating=None, iterative_triplet=False):\n",
        "        super().__init__(root)\n",
        "        self.min_rating = min_rating\n",
        "        self.min_uc = min_uc\n",
        "        self.min_sc = min_sc\n",
        "        self.split = split\n",
        "        self.dataset_split_seed = dataset_split_seed\n",
        "        self.eval_set_size = eval_set_size\n",
        "        self.iterative_triplet = iterative_triplet\n",
        "\n",
        "        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n",
        "\n",
        "        self._process()\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        r\"\"\"load raw dataset into pandas dataframe\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_processed(self):\n",
        "        return pickle.load(open(self.processed_paths[0], 'rb'))\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'dataset.pkl'\n",
        "\n",
        "    def make_implicit(self, df):\n",
        "        print('Turning into implicit ratings')\n",
        "        df = df[df['rating'] >= self.min_rating]\n",
        "        # return df[['uid', 'sid', 'timestamp']]\n",
        "        return df\n",
        "\n",
        "    def filter_triplets(self, df):\n",
        "        print('Filtering triplets')\n",
        "        if self.min_sc > 0:\n",
        "            item_sizes = df.groupby('sid').size()\n",
        "            good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
        "            df = df[df['sid'].isin(good_items)]\n",
        "\n",
        "        if self.min_uc > 0:\n",
        "            user_sizes = df.groupby('uid').size()\n",
        "            good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
        "            df = df[df['uid'].isin(good_users)]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def filter_triplets_iteratively(self, df):\n",
        "        print('Filtering triplets')\n",
        "        if self.min_sc > 0 or self.min_uc > 0:\n",
        "            item_sizes = df.groupby('sid').size()\n",
        "            good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
        "            user_sizes = df.groupby('uid').size()\n",
        "            good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
        "            while len(good_items) < len(item_sizes) or len(good_users) < len(user_sizes):\n",
        "                if self.min_sc > 0:\n",
        "                    item_sizes = df.groupby('sid').size()\n",
        "                    good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
        "                    df = df[df['sid'].isin(good_items)]\n",
        "\n",
        "                if self.min_uc > 0:\n",
        "                    user_sizes = df.groupby('uid').size()\n",
        "                    good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
        "                    df = df[df['uid'].isin(good_users)]\n",
        "\n",
        "                item_sizes = df.groupby('sid').size()\n",
        "                good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
        "                user_sizes = df.groupby('uid').size()\n",
        "                good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
        "            \n",
        "        return df\n",
        "\n",
        "    def densify_index(self, df):\n",
        "        print('Densifying index')\n",
        "        umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
        "        smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
        "        df['uid'] = df['uid'].map(umap)\n",
        "        df['sid'] = df['sid'].map(smap)\n",
        "        return df, umap, smap\n",
        "\n",
        "    def split_df(self, df, user_count):\n",
        "        if self.split == 'leave_one_out':\n",
        "            print('Splitting')\n",
        "            user_group = df.groupby('uid')\n",
        "            user2items = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['sid']))\n",
        "            train, val, test = {}, {}, {}\n",
        "            for user in range(user_count):\n",
        "                items = user2items[user]\n",
        "                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
        "            return train, val, test\n",
        "        elif self.split == 'holdout':\n",
        "            print('Splitting')\n",
        "            np.random.seed(self.dataset_split_seed)\n",
        "            eval_set_size = self.eval_set_size\n",
        "\n",
        "            # Generate user indices\n",
        "            permuted_index = np.random.permutation(user_count)\n",
        "            train_user_index = permuted_index[                :-2*eval_set_size]\n",
        "            val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
        "            test_user_index  = permuted_index[  -eval_set_size:                ]\n",
        "\n",
        "            # Split DataFrames\n",
        "            train_df = df.loc[df['uid'].isin(train_user_index)]\n",
        "            val_df   = df.loc[df['uid'].isin(val_user_index)]\n",
        "            test_df  = df.loc[df['uid'].isin(test_user_index)]\n",
        "\n",
        "            # DataFrame to dict => {uid : list of sid's}\n",
        "            train = dict(train_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
        "            val   = dict(val_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
        "            test  = dict(test_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
        "            return train, val, test\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def process(self):\n",
        "        df = self.load_ratings_df()\n",
        "        if self.iterative_triplet:\n",
        "            df = self.filter_triplets_iteratively(df)\n",
        "        else:\n",
        "            df = self.filter_triplets(df)\n",
        "        df, umap, smap = self.densify_index(df)\n",
        "        train, val, test = self.split_df(df, len(umap))\n",
        "        dataset = {'train': train,\n",
        "                   'val': val,\n",
        "                   'test': test,\n",
        "                   'umap': umap,\n",
        "                   'smap': smap}\n",
        "        with open(self.processed_paths[0], 'wb') as f:\n",
        "            pickle.dump(dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "oALw3tSngHZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RYej-omkeRJa"
      },
      "outputs": [],
      "source": [
        "class AmazonGamesDataset(RatingDataset):\n",
        "    url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'ratings_Video_Games.csv'\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.url, self.raw_dir)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        df = pd.read_csv(self.raw_paths[0], header=None)\n",
        "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yqh1JMC3eRJb",
        "outputId": "c9617c8d-7012-46c3-b5e7-7bdc4b3a2563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering triplets\n",
            "Densifying index\n",
            "Splitting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7519/7519 [00:05<00:00, 1295.99it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds = AmazonGamesDataset(root='/content/amazon_games', min_uc=10, min_sc=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3aPbNS1eRJc"
      },
      "outputs": [],
      "source": [
        "class AmazonBeautyDataset(RatingDataset):\n",
        "    url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Beauty.csv\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'ratings_Beauty.csv'\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.url, self.raw_dir)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        df = pd.read_csv(self.raw_paths[0], header=None)\n",
        "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZi9haBVeRJc",
        "outputId": "c529920e-7f5f-4820-f8cc-dcacf0c01072"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering triplets\n",
            "Densifying index\n",
            "Splitting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7519/7519 [00:02<00:00, 2527.69it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds = AmazonGamesDataset(root='/content/amazon_beauty', min_uc=10, min_sc=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hMgShp8eRJd"
      },
      "outputs": [],
      "source": [
        "class ML1mDataset(RatingDataset):\n",
        "    url = \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'ratings.dat'\n",
        "\n",
        "    def download(self):\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        from shutil import move, rmtree\n",
        "        move(os.path.join(self.raw_dir, 'ml-1m', self.raw_file_names), self.raw_dir)\n",
        "        rmtree(os.path.join(self.raw_dir, 'ml-1m'))\n",
        "        os.unlink(path)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        df = pd.read_csv(self.raw_paths[0], sep='::', header=None, engine='python')\n",
        "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmYznLTpeRJd",
        "outputId": "6d9bddcf-e835-4a69-d21e-6cd58adca354"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering triplets\n",
            "Densifying index\n",
            "Splitting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6040/6040 [00:02<00:00, 2590.97it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds = ML1mDataset(root='/content/ML1m', min_uc=10, min_sc=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy6EwQNfeRJe"
      },
      "outputs": [],
      "source": [
        "class SteamGamesDataset(RatingDataset):\n",
        "    url = \"http://cseweb.ucsd.edu/~wckang/steam_reviews.json.gz\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'steam_reviews.json'\n",
        "\n",
        "    def download(self):\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_gz(path, self.raw_dir)\n",
        "        os.unlink(path)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        data = []\n",
        "        f = open(self.raw_paths[0], 'r', encoding='utf-8')\n",
        "        import ast\n",
        "        for line in f.readlines():\n",
        "            temp = ast.literal_eval(line)\n",
        "            data.append([temp['username'], temp['product_id'], temp['date']])\n",
        "\n",
        "        return pd.DataFrame(data, columns=['uid', 'sid', 'timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYOWkU9keRJe",
        "outputId": "4a326b75-8015-4792-9ffb-9b0c040028e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering triplets\n",
            "Densifying index\n",
            "Splitting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 120145/120145 [01:10<00:00, 1709.62it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds = SteamGamesDataset(root='/content/steam', min_uc=10, min_sc=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAZDCcwBeRJf"
      },
      "outputs": [],
      "source": [
        "class YoochooseDataset(RatingDataset):\n",
        "    url = \"https://s3-eu-west-1.amazonaws.com/yc-rdata/yoochoose-data.7z\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'yoochoose-clicks.dat'\n",
        "\n",
        "    def download(self):\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        # pip install pyunpack patool\n",
        "        import pyunpack\n",
        "        pyunpack.Archive(path).extractall(self.raw_dir)\n",
        "        os.unlink(path)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        df = pd.read_csv(self.raw_paths[0], header=None)\n",
        "        df.columns = ['uid', 'timestamp', 'sid', 'category']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HrDS6z7eRJf",
        "outputId": "281df989-92ed-4f71-c52e-4e6edd6d9782"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering triplets\n",
            "Densifying index\n",
            "Splitting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 449961/449961 [03:55<00:00, 1913.52it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds = YoochooseDataset(root='/content/yoochoose', min_uc=10, min_sc=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234XjJBUeRJk"
      },
      "source": [
        "## Torch Rating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usEJErF0eRJl"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class TorchRatingDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
        "        \"\"\"Init UserItemRatingDataset Class.\n",
        "        Args:\n",
        "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair.\n",
        "        \"\"\"\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get an item from dataset.\"\"\"\n",
        "        return (\n",
        "            self.user_tensor[index],\n",
        "            self.item_tensor[index],\n",
        "            self.target_tensor[index],\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get the size of the dataset.\"\"\"\n",
        "        return self.user_tensor.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ava5MX0geRJl"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class TorchPairwiseNegativeDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Wrapper, convert <user, pos_item, neg_item> Tensor into Pytorch Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, user_tensor, pos_item_tensor, neg_item_tensor):\n",
        "        \"\"\"Init PairwiseNegativeDataset Class.\n",
        "        Args:\n",
        "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair.\n",
        "        \"\"\"\n",
        "        self.user_tensor = user_tensor\n",
        "        self.pos_item_tensor = pos_item_tensor\n",
        "        self.neg_item_tensor = neg_item_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get an item from the dataset.\"\"\"\n",
        "        return (\n",
        "            self.user_tensor[index],\n",
        "            self.pos_item_tensor[index],\n",
        "            self.neg_item_tensor[index],\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get the size of the dataset.\"\"\"\n",
        "        return self.user_tensor.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm6eJTzyeRJm"
      },
      "source": [
        "> **References:-**\n",
        "- https://github.com/beta-team/beta-recsys/blob/master/beta_rec/data/data_loaders.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw6SN-o7eRJm",
        "outputId": "1d93285c-2679-4e30-f490-891dc6a6a305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2021-12-31 06:27:08\n",
            "\n",
            "recohut: 0.0.8\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "sys    : 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "numpy  : 1.19.5\n",
            "pandas : 1.1.5\n",
            "csv    : 1.0\n",
            "IPython: 5.5.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "datasets > bases > ratings",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}