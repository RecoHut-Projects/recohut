{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBwvLL5kIvFm"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.avazu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH9uj-ktIvFw"
      },
      "source": [
        "# Avazu\n",
        "> Avazu dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dKA9N4UIvF3"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "from recohut.datasets.bases.ctr import *\n",
        "from recohut.utils.common_utils import download_url\n",
        "\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "S2yArBpGFnvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class AvazuDataset(CTRDataset):\n",
        "\n",
        "    feature_cols = [{'name': 'id', 'active': False, 'dtype': 'str', 'type': 'categorical'},\n",
        "         {'name': 'hour', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_hour'},\n",
        "         {'name': ['C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id',\n",
        "                   'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21'], \n",
        "           'active': True, 'dtype': 'str', 'type': 'categorical'},\n",
        "         {'name': 'weekday', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_weekday'},\n",
        "         {'name': 'weekend', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_weekend'}]\n",
        "                        \n",
        "    label_col = {'name': 'click', 'dtype': float}\n",
        "\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/train.csv\"\n",
        "    valid_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/valid.csv\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/test.csv\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['train.csv',\n",
        "                'valid.csv',\n",
        "                'test.csv']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.train_url, self.raw_dir)\n",
        "        download_url(self.valid_url, self.raw_dir)\n",
        "        download_url(self.test_url, self.raw_dir)\n",
        "\n",
        "    def convert_weekday(self, df, col_name):\n",
        "        def _convert_weekday(timestamp):\n",
        "            dt = date(int('20' + timestamp[0:2]), int(timestamp[2:4]), int(timestamp[4:6]))\n",
        "            return int(dt.strftime('%w'))\n",
        "        return df['hour'].apply(_convert_weekday)\n",
        "\n",
        "    def convert_weekend(self, df, col_name):\n",
        "        def _convert_weekend(timestamp):\n",
        "            dt = date(int('20' + timestamp[0:2]), int(timestamp[2:4]), int(timestamp[4:6]))\n",
        "            return 1 if dt.strftime('%w') in ['6', '0'] else 0\n",
        "        return df['hour'].apply(_convert_weekend)\n",
        "\n",
        "    def convert_hour(self, df, col_name):\n",
        "        return df['hour'].apply(lambda x: int(x[6:8]))"
      ],
      "metadata": {
        "id": "dWKBFeOTB81K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class AvazuDataModule(CTRDataModule):\n",
        "    dataset_cls = AvazuDataset"
      ],
      "metadata": {
        "id": "imArN3L3E_Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "XM0hOfIjLdKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'model_id': 'DCN_demo',\n",
        "              'data_dir': '/content/data',\n",
        "              'model_root': './checkpoints/',\n",
        "              'dnn_hidden_units': [64, 64],\n",
        "              'dnn_activations': \"relu\",\n",
        "              'crossing_layers': 3,\n",
        "              'learning_rate': 1e-3,\n",
        "              'net_dropout': 0,\n",
        "              'batch_norm': False,\n",
        "              'optimizer': 'adamw',\n",
        "              'task': 'binary_classification',\n",
        "              'loss': 'binary_crossentropy',\n",
        "              'metrics': ['logloss', 'AUC'],\n",
        "              'embedding_dim': 10,\n",
        "              'batch_size': 64,\n",
        "              'epochs': 3,\n",
        "              'shuffle': True,\n",
        "              'seed': 2019,\n",
        "              'use_hdf5': True,\n",
        "              'workers': 1,\n",
        "              'verbose': 0}"
      ],
      "metadata": {
        "id": "egWI7tWgVMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data\n",
        "ds = AvazuDataModule(**params)\n",
        "ds.prepare_data()\n",
        "ds.setup()\n",
        "\n",
        "for batch in ds.train_dataloader():\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm5J-aNOV3uC",
        "outputId": "7030c406-353c-4858-f67f-3bc7dcf71803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:74: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
            "  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n",
            "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/train.csv\n",
            "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/valid.csv\n",
            "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/test.csv\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[1., 1., 2.,  ..., 6., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [1., 1., 2.,  ..., 2., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 4., 1., 1.]], dtype=torch.float64), tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from recohut.models.deepcrossing import DeepCrossing\n",
        "\n",
        "params = {'model_id': 'DeepCrossing',\n",
        "              'data_dir': '/content/data',\n",
        "              'model_root': './checkpoints/',\n",
        "              'dnn_hidden_units': [64, 64],\n",
        "              'dnn_activations': \"relu\",\n",
        "              'learning_rate': 1e-3,\n",
        "              'net_dropout': 0,\n",
        "              'batch_norm': False,\n",
        "              'optimizer': 'adamw',\n",
        "              'use_residual': True,\n",
        "              'residual_blocks': [500, 500, 500],\n",
        "              'task': 'binary_classification',\n",
        "              'loss': 'binary_crossentropy',\n",
        "              'metrics': ['logloss', 'AUC'],\n",
        "              'embedding_dim': 10,\n",
        "              'batch_size': 64,\n",
        "              'epochs': 3,\n",
        "              'shuffle': True,\n",
        "              'seed': 2019,\n",
        "              'use_hdf5': True,\n",
        "              'workers': 1,\n",
        "              'verbose': 0}\n",
        "\n",
        "model = DeepCrossing(ds.dataset.feature_map, **params)"
      ],
      "metadata": {
        "id": "heU-ch948_5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from recohut.trainers.pl_trainer import pl_trainer\n",
        "\n",
        "pl_trainer(model, ds, max_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a251da889274457fa0e3625bf7cb16a5",
            "3f8be23d064e48d78fd1e6229b59d115",
            "83404db397bc44eebebb38c26baf98bd",
            "238e57d0fc6346c49d1133f75a1db7fc",
            "7f9558ffe19b422dbef8708f9beb7e8f",
            "63e409d18b214136b1ef876b311a80a6",
            "eb22abdd9ae64c8bbab975a45447210e",
            "7301ac3746ab46ee9dc8daa4e2298746",
            "2c725a342a294efba77df69cbe71b227",
            "8b16e99182bf42adab9b5bb737f9d45c",
            "a8acf1c798494270aa5970dd821c40a9",
            "393968cbaf7e404fa8f7aebb5219c6cf",
            "a21834d2207c4089a491df3ddb418663",
            "71531362a256430dbd36cc87ddf13fb0",
            "96b667eb3ac84e91a237b409bf6b56c7",
            "d76dad1ce84e428285743e165c045609",
            "cfb779c0d6634dd8b7c25ab8e260991e",
            "831ed868399b468ba1671baa487c3902",
            "57866549cdf94a6fa06d8d56a57cc5f6",
            "a7609d140d7343da9269b3630a6bd6a7",
            "88c8a51b51fc405d85cf9813ec9770c0",
            "eb7a0649ae22414daee53b99843892ba",
            "d559374f05874adf9e94b504a3cd4a90",
            "382d1e90849547cb9c002ec9700d5266",
            "66af5d64b41b4e97afde2c397e68cb5f",
            "a85f1970861d4206bf3059f28ca15d9e",
            "dbea1b6eda484b18b9d6b6f1984b4e03",
            "8caa4dce9b034066a8c2c18131f2934a",
            "07e275275c4f44a1a02aac8e4c2916c0",
            "fd63bbcd423a43f4b26c4930fb0b77f5",
            "e3a7c6bd7b844cefacf7e1a2356d41b2",
            "04a5eedc48ac4fc6a881dde335301f9c",
            "0049f970eac94747ad82e01bf44b75b1"
          ]
        },
        "id": "dTacyen5HGrn",
        "outputId": "e22e0fcb-a47d-41af-fc97-a3375ca92790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
            "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
            "\n",
            "  | Name              | Type           | Params\n",
            "-----------------------------------------------------\n",
            "0 | embedding_layer   | EmbeddingLayer | 12.6 K\n",
            "1 | crossing_layer    | Sequential     | 722 K \n",
            "2 | output_activation | Sigmoid        | 0     \n",
            "-----------------------------------------------------\n",
            "735 K     Trainable params\n",
            "0         Non-trainable params\n",
            "735 K     Total params\n",
            "2.940     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /content exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a251da889274457fa0e3625bf7cb16a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "393968cbaf7e404fa8f7aebb5219c6cf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
            "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d559374f05874adf9e94b504a3cd4a90",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(1.0680e-07)}}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(1.0680e-07)}}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvdjsncbIvF6"
      },
      "outputs": [],
      "source": [
        "# #export\n",
        "# import shutil\n",
        "# import struct\n",
        "# from collections import defaultdict\n",
        "# from pathlib import Path\n",
        "\n",
        "# import lmdb\n",
        "# import numpy as np\n",
        "# import torch.utils.data\n",
        "# from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNlktQIFIvF8"
      },
      "outputs": [],
      "source": [
        "# #export\n",
        "# class AvazuDataset(torch.utils.data.Dataset):\n",
        "#     \"\"\"\n",
        "#     Avazu Click-Through Rate Prediction Dataset\n",
        "#     Dataset preparation\n",
        "#         Remove the infrequent features (appearing in less than threshold instances) and treat them as a single feature\n",
        "#     :param dataset_path: avazu train path\n",
        "#     :param cache_path: lmdb cache path\n",
        "#     :param rebuild_cache: If True, lmdb cache is refreshed\n",
        "#     :param min_threshold: infrequent feature threshold\n",
        "#     Reference\n",
        "#         https://www.kaggle.com/c/avazu-ctr-prediction\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, dataset_path=None, cache_path='.avazu', rebuild_cache=False, min_threshold=4):\n",
        "#         self.NUM_FEATS = 22\n",
        "#         self.min_threshold = min_threshold\n",
        "#         if rebuild_cache or not Path(cache_path).exists():\n",
        "#             shutil.rmtree(cache_path, ignore_errors=True)\n",
        "#             if dataset_path is None:\n",
        "#                 raise ValueError('create cache: failed: dataset_path is None')\n",
        "#             self.__build_cache(dataset_path, cache_path)\n",
        "#         self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
        "#         with self.env.begin(write=False) as txn:\n",
        "#             self.length = txn.stat()['entries'] - 1\n",
        "#             self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint32)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         with self.env.begin(write=False) as txn:\n",
        "#             np_array = np.frombuffer(\n",
        "#                 txn.get(struct.pack('>I', index)), dtype=np.uint32).astype(dtype=np.long)\n",
        "#         return np_array[1:], np_array[0]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.length\n",
        "\n",
        "#     def __build_cache(self, path, cache_path):\n",
        "#         feat_mapper, defaults = self.__get_feat_mapper(path)\n",
        "#         with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
        "#             field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint32)\n",
        "#             for i, fm in feat_mapper.items():\n",
        "#                 field_dims[i - 1] = len(fm) + 1\n",
        "#             with env.begin(write=True) as txn:\n",
        "#                 txn.put(b'field_dims', field_dims.tobytes())\n",
        "#             for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
        "#                 with env.begin(write=True) as txn:\n",
        "#                     for key, value in buffer:\n",
        "#                         txn.put(key, value)\n",
        "\n",
        "#     def __get_feat_mapper(self, path):\n",
        "#         feat_cnts = defaultdict(lambda: defaultdict(int))\n",
        "#         with open(path) as f:\n",
        "#             f.readline()\n",
        "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
        "#             pbar.set_description('Create avazu dataset cache: counting features')\n",
        "#             for line in pbar:\n",
        "#                 values = line.rstrip('\\n').split(',')\n",
        "#                 if len(values) != self.NUM_FEATS + 2:\n",
        "#                     continue\n",
        "#                 for i in range(1, self.NUM_FEATS + 1):\n",
        "#                     feat_cnts[i][values[i + 1]] += 1\n",
        "#         feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()}\n",
        "#         feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
        "#         defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
        "#         return feat_mapper, defaults\n",
        "\n",
        "#     def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
        "#         item_idx = 0\n",
        "#         buffer = list()\n",
        "#         with open(path) as f:\n",
        "#             f.readline()\n",
        "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
        "#             pbar.set_description('Create avazu dataset cache: setup lmdb')\n",
        "#             for line in pbar:\n",
        "#                 values = line.rstrip('\\n').split(',')\n",
        "#                 if len(values) != self.NUM_FEATS + 2:\n",
        "#                     continue\n",
        "#                 np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint32)\n",
        "#                 np_array[0] = int(values[1])\n",
        "#                 for i in range(1, self.NUM_FEATS + 1):\n",
        "#                     np_array[i] = feat_mapper[i].get(values[i+1], defaults[i])\n",
        "#                 buffer.append((struct.pack('>I', item_idx), np_array.tobytes()))\n",
        "#                 item_idx += 1\n",
        "#                 if item_idx % buffer_size == 0:\n",
        "#                     yield buffer\n",
        "#                     buffer.clear()\n",
        "#             yield buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJcMx0uYIvGH"
      },
      "source": [
        "> **References:-**\n",
        "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/dataset/avazu.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut,pytorch_lightning"
      ],
      "metadata": {
        "id": "d7Iqbq8K9lqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6e9509-3238-4386-eee5-d7dc7a8292d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-11 22:08:24\n",
            "\n",
            "recohut          : 0.0.11\n",
            "pytorch_lightning: 1.5.8\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "IPython: 5.5.0\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "datasets.avazu.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}