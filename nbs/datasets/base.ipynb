{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Dataset\n",
    "> Abstract base class for dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from abc import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def download(url, savepath):\n",
    "    import wget\n",
    "    wget.download(url, str(savepath))\n",
    "\n",
    "\n",
    "def unzip(zippath, savepath):\n",
    "    import zipfile\n",
    "    zip = zipfile.ZipFile(zippath)\n",
    "    zip.extractall(savepath)\n",
    "    zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbstractDataset(metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        self.min_rating = args.min_rating\n",
    "        self.min_uc = args.min_uc\n",
    "        self.min_sc = args.min_sc\n",
    "        self.split = args.split\n",
    "        self.RAW_DATASET_ROOT_FOLDER = args.RAW_DATASET_ROOT_FOLDER\n",
    "        self.PREP_DATASET_ROOT_FOLDER = args.PREP_DATASET_ROOT_FOLDER\n",
    "\n",
    "        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def code(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def raw_code(cls):\n",
    "        return cls.code()\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_ratings_df(self):\n",
    "        pass\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.preprocess()\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        dataset = pickle.load(dataset_path.open('rb'))\n",
    "        return dataset\n",
    "\n",
    "    def filter_triplets(self, df):\n",
    "        print('Filtering triplets')\n",
    "        if self.min_sc > 0:\n",
    "            item_sizes = df.groupby('sid').size()\n",
    "            good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
    "            df = df[df['sid'].isin(good_items)]\n",
    "\n",
    "        if self.min_uc > 0:\n",
    "            user_sizes = df.groupby('uid').size()\n",
    "            good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
    "            df = df[df['uid'].isin(good_users)]\n",
    "        return df\n",
    "\n",
    "    def densify_index(self, df):\n",
    "        print('Densifying index')\n",
    "        umap = {u: i for i, u in enumerate(set(df['uid']), start=1)}\n",
    "        smap = {s: i for i, s in enumerate(set(df['sid']), start=1)}\n",
    "        df['uid'] = df['uid'].map(umap)\n",
    "        df['sid'] = df['sid'].map(smap)\n",
    "        return df, umap, smap\n",
    "\n",
    "    def split_df(self, df, user_count):\n",
    "        if self.split == 'leave_one_out':\n",
    "            print('Splitting')\n",
    "            user_group = df.groupby('uid')\n",
    "            user2items = user_group.progress_apply(\n",
    "                lambda d: list(d.sort_values(by=['timestamp', 'sid'])['sid']))\n",
    "            train, val, test = {}, {}, {}\n",
    "            for i in range(user_count):\n",
    "                user = i + 1\n",
    "                items = user2items[user]\n",
    "                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "            return train, val, test\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _get_rawdata_root_path(self):\n",
    "        return Path(self.RAW_DATASET_ROOT_FOLDER)\n",
    "\n",
    "    def _get_rawdata_folder_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath(self.raw_code())\n",
    "\n",
    "    def _get_preprocessed_root_path(self):\n",
    "        root = Path(self.PREP_DATASET_ROOT_FOLDER)\n",
    "        return root.joinpath(self.raw_code())\n",
    "\n",
    "    def _get_preprocessed_folder_path(self):\n",
    "        preprocessed_root = self._get_preprocessed_root_path()\n",
    "        # folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n",
    "        #     .format(self.code(), self.min_rating, self.min_uc, self.min_sc, self.split)\n",
    "        # return preprocessed_root.joinpath(folder_name)\n",
    "        return preprocessed_root\n",
    "\n",
    "    def _get_preprocessed_dataset_path(self):\n",
    "        folder = self._get_preprocessed_folder_path()\n",
    "        return folder.joinpath('dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbstractDatasetv2(metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.min_rating = args.min_rating\n",
    "        self.min_uc = args.min_uc\n",
    "        self.min_sc = args.min_sc\n",
    "        self.split = args.split\n",
    "\n",
    "        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def code(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def raw_code(cls):\n",
    "        return cls.code()\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def url(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def is_zipfile(cls):\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def zip_file_content_is_folder(cls):\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def all_raw_file_names(cls):\n",
    "        return []\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_ratings_df(self):\n",
    "        pass\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.preprocess()\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        dataset = pickle.load(dataset_path.open('rb'))\n",
    "        return dataset\n",
    "\n",
    "    def preprocess(self):\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        if dataset_path.is_file():\n",
    "            print('Already preprocessed. Skip preprocessing')\n",
    "            return\n",
    "        if not dataset_path.parent.is_dir():\n",
    "            dataset_path.parent.mkdir(parents=True)\n",
    "        self.maybe_download_raw_dataset()\n",
    "        df = self.load_ratings_df()\n",
    "        df = self.make_implicit(df)\n",
    "        df = self.filter_triplets(df)\n",
    "        df, umap, smap = self.densify_index(df)\n",
    "        train, val, test = self.split_df(df, len(umap))\n",
    "        dataset = {'train': train,\n",
    "                   'val': val,\n",
    "                   'test': test,\n",
    "                   'umap': umap,\n",
    "                   'smap': smap}\n",
    "        with dataset_path.open('wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    def maybe_download_raw_dataset(self):\n",
    "        folder_path = self._get_rawdata_folder_path()\n",
    "        if folder_path.is_dir() and\\\n",
    "           all(folder_path.joinpath(filename).is_file() for filename in self.all_raw_file_names()):\n",
    "            print('Raw data already exists. Skip downloading')\n",
    "            return\n",
    "        print(\"Raw file doesn't exist. Downloading...\")\n",
    "        if self.is_zipfile():\n",
    "            tmproot = Path(tempfile.mkdtemp())\n",
    "            tmpzip = tmproot.joinpath('file.zip')\n",
    "            tmpfolder = tmproot.joinpath('folder')\n",
    "            download(self.url(), tmpzip)\n",
    "            unzip(tmpzip, tmpfolder)\n",
    "            if self.zip_file_content_is_folder():\n",
    "                tmpfolder = tmpfolder.joinpath(os.listdir(tmpfolder)[0])\n",
    "            shutil.move(tmpfolder, folder_path)\n",
    "            shutil.rmtree(tmproot)\n",
    "            print()\n",
    "        else:\n",
    "            tmproot = Path(tempfile.mkdtemp())\n",
    "            tmpfile = tmproot.joinpath('file')\n",
    "            download(self.url(), tmpfile)\n",
    "            folder_path.mkdir(parents=True)\n",
    "            shutil.move(tmpfile, folder_path.joinpath('ratings.csv'))\n",
    "            shutil.rmtree(tmproot)\n",
    "            print()\n",
    "\n",
    "    def make_implicit(self, df):\n",
    "        print('Turning into implicit ratings')\n",
    "        df = df[df['rating'] >= self.min_rating]\n",
    "        # return df[['uid', 'sid', 'timestamp']]\n",
    "        return df\n",
    "\n",
    "    def filter_triplets(self, df):\n",
    "        print('Filtering triplets')\n",
    "        if self.min_sc > 0:\n",
    "            item_sizes = df.groupby('sid').size()\n",
    "            good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
    "            df = df[df['sid'].isin(good_items)]\n",
    "\n",
    "        if self.min_uc > 0:\n",
    "            user_sizes = df.groupby('uid').size()\n",
    "            good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
    "            df = df[df['uid'].isin(good_users)]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def densify_index(self, df):\n",
    "        print('Densifying index')\n",
    "        umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "        smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "        df['uid'] = df['uid'].map(umap)\n",
    "        df['sid'] = df['sid'].map(smap)\n",
    "        return df, umap, smap\n",
    "\n",
    "    def split_df(self, df, user_count):\n",
    "        if self.args.split == 'leave_one_out':\n",
    "            print('Splitting')\n",
    "            user_group = df.groupby('uid')\n",
    "            user2items = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['sid']))\n",
    "            train, val, test = {}, {}, {}\n",
    "            for user in range(user_count):\n",
    "                items = user2items[user]\n",
    "                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "            return train, val, test\n",
    "        elif self.args.split == 'holdout':\n",
    "            print('Splitting')\n",
    "            np.random.seed(self.args.dataset_split_seed)\n",
    "            eval_set_size = self.args.eval_set_size\n",
    "\n",
    "            # Generate user indices\n",
    "            permuted_index = np.random.permutation(user_count)\n",
    "            train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "            val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "            test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "            # Split DataFrames\n",
    "            train_df = df.loc[df['uid'].isin(train_user_index)]\n",
    "            val_df   = df.loc[df['uid'].isin(val_user_index)]\n",
    "            test_df  = df.loc[df['uid'].isin(test_user_index)]\n",
    "\n",
    "            # DataFrame to dict => {uid : list of sid's}\n",
    "            train = dict(train_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            val   = dict(val_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            test  = dict(test_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            return train, val, test\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _get_rawdata_root_path(self):\n",
    "        return Path(self.args.RAW_DATASET_ROOT_FOLDER)\n",
    "\n",
    "    def _get_rawdata_folder_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath(self.raw_code())\n",
    "\n",
    "    def _get_preprocessed_root_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath('preprocessed')\n",
    "\n",
    "    def _get_preprocessed_folder_path(self):\n",
    "        preprocessed_root = self._get_preprocessed_root_path()\n",
    "        folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n",
    "            .format(self.code(), self.min_rating, self.min_uc, self.min_sc, self.split)\n",
    "        return preprocessed_root.joinpath(folder_name)\n",
    "\n",
    "    def _get_preprocessed_dataset_path(self):\n",
    "        folder = self._get_preprocessed_folder_path()\n",
    "        return folder.joinpath('dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2021-12-18 09:38:50\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.104+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "IPython: 5.5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install -q watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
