{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M234136 | layers > layers.common",
      "provenance": [],
      "collapsed_sections": [
        "1KypvcFZI64_"
      ],
      "mount_file_id": "1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a",
      "authorship_tag": "ABX9TyMgM2A3MDat4nZhbn/NIWrR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# default_exp layers.common"
      ],
      "metadata": {
        "id": "sLigaYYZhW1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Layers\n",
        "> Common layers."
      ],
      "metadata": {
        "id": "Yt-iXgBmuY6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3eJI-JBuJzJ"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37L_fwkDuJzK"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class FeaturesLinear(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias"
      ],
      "metadata": {
        "id": "LVDBXB07uJzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class FeaturesEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return self.embedding(x)"
      ],
      "metadata": {
        "id": "xkny_R6YS2x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class MultiLayerPerceptron(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
        "        super().__init__()\n",
        "        layers = list()\n",
        "        for embed_dim in embed_dims:\n",
        "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
        "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "            layers.append(torch.nn.Dropout(p=dropout))\n",
        "            input_dim = embed_dim\n",
        "        if output_layer:\n",
        "            layers.append(torch.nn.Linear(input_dim, 1))\n",
        "        self.mlp = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
        "        \"\"\"\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "Ew0ZCHUOTBC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}