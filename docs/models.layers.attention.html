---

title: Attention Layers


keywords: fastai
sidebar: home_sidebar

summary: "Implementation of Attention modules including Multihead attention etc.."
description: "Implementation of Attention modules including Multihead attention etc.."
nb_path: "nbs/models/layers/models.layers.attention.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/models/layers/models.layers.attention.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TokenEmbedding">TokenEmbedding<a class="anchor-link" href="#TokenEmbedding"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TokenEmbedding" class="doc_header"><code>class</code> <code>TokenEmbedding</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TokenEmbedding</code>(<strong><code>vocab_size</code></strong>, <strong><code>embed_size</code></strong>=<em><code>512</code></em>) :: <code>Embedding</code></p>
</blockquote>
<p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<p>Args:
    num_embeddings (int): size of the dictionary of embeddings
    embedding_dim (int): the size of each embedding vector
    padding_idx (int, optional): If specified, the entries at :attr:<code>padding_idx</code> do not contribute to the gradient;
                                 therefore, the embedding vector at :attr:<code>padding_idx</code> is not updated during training,
                                 i.e. it remains as a fixed "pad". For a newly constructed Embedding,
                                 the embedding vector at :attr:<code>padding_idx</code> will default to all zeros,
                                 but can be updated to another value to be used as the padding vector.
    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:<code>max_norm</code>
                                is renormalized to have norm :attr:<code>max_norm</code>.
    norm_type (float, optional): The p of the p-norm to compute for the :attr:<code>max_norm</code> option. Default <code>2</code>.
    scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                            the words in the mini-batch. Default <code>False</code>.
    sparse (bool, optional): If <code>True</code>, gradient w.r.t. :attr:<code>weight</code> matrix will be a sparse tensor.
                             See Notes for more details regarding sparse gradients.</p>
<p>Attributes:
    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)
                     initialized from :math:<code>\mathcal{N}(0, 1)</code></p>
<p>Shape:</p>

<pre><code>- Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract
- Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\text{embedding\_dim}`

</code></pre>
<p>.. note::
    Keep in mind that only a limited number of optimizers support
    sparse gradients: currently it's :class:<code>optim.SGD</code> (<code>CUDA</code> and <code>CPU</code>),
    :class:<code>optim.SparseAdam</code> (<code>CUDA</code> and <code>CPU</code>) and :class:<code>optim.Adagrad</code> (<code>CPU</code>)</p>
<p>.. note::
    When :attr:<code>max_norm</code> is not <code>None</code>, :class:<code>Embedding</code>'s forward method will modify the
    :attr:<code>weight</code> tensor in-place. Since tensors needed for gradient computations cannot be
    modified in-place, performing a differentiable operation on <code>Embedding.weight</code> before
    calling :class:<code>Embedding</code>'s forward method requires cloning <code>Embedding.weight</code> when
    :attr:<code>max_norm</code> is not <code>None</code>. For example::</p>

<pre><code>    n, d, m = 3, 5, 7
    embedding = nn.Embedding(n, d, max_norm=True)
    W = torch.randn((m, d), requires_grad=True)
    idx = torch.tensor([1, 2])
    a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable
    b = embedding(idx) @ W.t()  # modifies weight in-place
    out = (a.unsqueeze(0) + b.unsqueeze(1))
    loss = out.sigmoid().prod()
    loss.backward()

</code></pre>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding = nn.Embedding(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
&gt;&gt;&gt; embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])


&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)
&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])
&gt;&gt;&gt; embedding(input)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.1535, -2.0309,  0.9315],
         [ 0.0000,  0.0000,  0.0000],
         [-0.1655,  0.9897,  0.0635]]])

&gt;&gt;&gt; # example of changing `pad` vector
&gt;&gt;&gt; padding_idx = 0
&gt;&gt;&gt; embedding = nn.Embedding(3, 3, padding_idx=padding_idx)
&gt;&gt;&gt; embedding.weight
Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000],
        [-0.7895, -0.7089, -0.0364],
        [ 0.6778,  0.5803,  0.2678]], requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     embedding.weight[padding_idx] = torch.ones(3)
&gt;&gt;&gt; embedding.weight
Parameter containing:
tensor([[ 1.0000,  1.0000,  1.0000],
        [-0.7895, -0.7089, -0.0364],
        [ 0.6778,  0.5803,  0.2678]], requires_grad=True)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PositionalEmbedding">PositionalEmbedding<a class="anchor-link" href="#PositionalEmbedding"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PositionalEmbedding" class="doc_header"><code>class</code> <code>PositionalEmbedding</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PositionalEmbedding</code>(<strong><code>max_len</code></strong>, <strong><code>d_model</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GELU" class="doc_header"><code>class</code> <code>GELU</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L37" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GELU</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PositionwiseFeedForward" class="doc_header"><code>class</code> <code>PositionwiseFeedForward</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L42" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PositionwiseFeedForward</code>(<strong><code>d_model</code></strong>, <strong><code>d_ff</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LayerNorm" class="doc_header"><code>class</code> <code>LayerNorm</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L53" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LayerNorm</code>(<strong><code>features</code></strong>, <strong><code>eps</code></strong>=<em><code>1e-06</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SublayerConnection" class="doc_header"><code>class</code> <code>SublayerConnection</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L66" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SublayerConnection</code>(<strong><code>size</code></strong>, <strong><code>dropout</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>layer norm and dropout (dropout and then layer norm)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Attention">Attention<a class="anchor-link" href="#Attention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Attention" class="doc_header"><code>class</code> <code>Attention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L79" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Attention</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ScaledDotProductAttention">ScaledDotProductAttention<a class="anchor-link" href="#ScaledDotProductAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ScaledDotProductAttention" class="doc_header"><code>class</code> <code>ScaledDotProductAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L100" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ScaledDotProductAttention</code>(<strong><code>dropout_rate</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Scaled Dot-Product Attention
Ref: <a href="https://zhuanlan.zhihu.com/p/47812375">https://zhuanlan.zhihu.com/p/47812375</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MultiHeadAttention">MultiHeadAttention<a class="anchor-link" href="#MultiHeadAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MultiHeadAttention" class="doc_header"><code>class</code> <code>MultiHeadAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L124" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MultiHeadAttention</code>(<strong><code>input_dim</code></strong>, <strong><code>attention_dim</code></strong>=<em><code>None</code></em>, <strong><code>num_heads</code></strong>=<em><code>1</code></em>, <strong><code>dropout_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>use_residual</code></strong>=<em><code>True</code></em>, <strong><code>use_scale</code></strong>=<em><code>False</code></em>, <strong><code>layer_norm</code></strong>=<em><code>False</code></em>, <strong><code>align_to</code></strong>=<em><code>'input'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Multi-head attention module</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MultiHeadedAttention_v2">MultiHeadedAttention_v2<a class="anchor-link" href="#MultiHeadedAttention_v2"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MultiHeadedAttention_v2" class="doc_header"><code>class</code> <code>MultiHeadedAttention_v2</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L187" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MultiHeadedAttention_v2</code>(<strong><code>h</code></strong>, <strong><code>d_model</code></strong>, <strong><code>head_size</code></strong>=<em><code>None</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MultiHeadSelfAttention">MultiHeadSelfAttention<a class="anchor-link" href="#MultiHeadSelfAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MultiHeadSelfAttention" class="doc_header"><code>class</code> <code>MultiHeadSelfAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L222" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MultiHeadSelfAttention</code>(<strong><code>input_dim</code></strong>, <strong><code>attention_dim</code></strong>=<em><code>None</code></em>, <strong><code>num_heads</code></strong>=<em><code>1</code></em>, <strong><code>dropout_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>use_residual</code></strong>=<em><code>True</code></em>, <strong><code>use_scale</code></strong>=<em><code>False</code></em>, <strong><code>layer_norm</code></strong>=<em><code>False</code></em>, <strong><code>align_to</code></strong>=<em><code>'input'</code></em>) :: <a href="/recohut/models.layers.attention.html#MultiHeadAttention"><code>MultiHeadAttention</code></a></p>
</blockquote>
<p>Multi-head attention module</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TransformerBlock">TransformerBlock<a class="anchor-link" href="#TransformerBlock"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformerBlock" class="doc_header"><code>class</code> <code>TransformerBlock</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L228" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformerBlock</code>(<strong><code>hidden</code></strong>, <strong><code>attn_heads</code></strong>, <strong><code>head_size</code></strong>, <strong><code>feed_forward_hidden</code></strong>, <strong><code>dropout</code></strong>, <strong><code>attn_dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SqueezeExcitationLayer">SqueezeExcitationLayer<a class="anchor-link" href="#SqueezeExcitationLayer"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SqueezeExcitationLayer" class="doc_header"><code>class</code> <code>SqueezeExcitationLayer</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L245" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SqueezeExcitationLayer</code>(<strong><code>num_fields</code></strong>, <strong><code>reduction_ratio</code></strong>=<em><code>3</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SASMultiHeadedAttention">SASMultiHeadedAttention<a class="anchor-link" href="#SASMultiHeadedAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SASMultiHeadedAttention" class="doc_header"><code>class</code> <code>SASMultiHeadedAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L261" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SASMultiHeadedAttention</code>(<strong><code>h</code></strong>, <strong><code>d_model</code></strong>, <strong><code>head_size</code></strong>=<em><code>None</code></em>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SASPositionwiseFeedForward" class="doc_header"><code>class</code> <code>SASPositionwiseFeedForward</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L297" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SASPositionwiseFeedForward</code>(<strong><code>d_model</code></strong>, <strong><code>d_ff</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SASTransformerBlock">SASTransformerBlock<a class="anchor-link" href="#SASTransformerBlock"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SASTransformerBlock" class="doc_header"><code>class</code> <code>SASTransformerBlock</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L311" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SASTransformerBlock</code>(<strong><code>hidden</code></strong>, <strong><code>attn_heads</code></strong>, <strong><code>head_size</code></strong>, <strong><code>feed_forward_hidden</code></strong>, <strong><code>dropout</code></strong>, <strong><code>attn_dropout</code></strong>=<em><code>0.1</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SelfAttention">SelfAttention<a class="anchor-link" href="#SelfAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SelfAttention" class="doc_header"><code>class</code> <code>SelfAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L326" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SelfAttention</code>(<strong><code>hidden_size</code></strong>, <strong><code>num_attention_heads</code></strong>, <strong><code>attention_probs_dropout_prob</code></strong>, <strong><code>hidden_dropout_prob</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>References:</p>

<pre><code>1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/modules.py#L127</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">hidden_dropout_prob</span><span class="p">,</span>
                      <span class="n">attention_probs_dropout_prob</span><span class="p">)</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="DistSelfAttention">DistSelfAttention<a class="anchor-link" href="#DistSelfAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistSelfAttention" class="doc_header"><code>class</code> <code>DistSelfAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L391" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistSelfAttention</code>(<strong><code>hidden_size</code></strong>, <strong><code>num_attention_heads</code></strong>, <strong><code>hidden_dropout_prob</code></strong>, <strong><code>attention_probs_dropout_prob</code></strong>, <strong><code>distance_metric</code></strong>=<em><code>'wasserstein'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">distance</span> <span class="o">=</span> <span class="s1">&#39;wasserstein&#39;</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">DistSelfAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">hidden_dropout_prob</span><span class="p">,</span>
                          <span class="n">attention_probs_dropout_prob</span><span class="p">,</span> <span class="n">distance</span><span class="p">)</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

<span class="n">mean_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">mean_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mean_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">cov_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">cov_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">cov_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="DistMeanSelfAttention">DistMeanSelfAttention<a class="anchor-link" href="#DistMeanSelfAttention"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistMeanSelfAttention" class="doc_header"><code>class</code> <code>DistMeanSelfAttention</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/layers/attention.py#L476" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistMeanSelfAttention</code>(<strong><code>hidden_size</code></strong>, <strong><code>num_attention_heads</code></strong>, <strong><code>attention_probs_dropout_prob</code></strong>, <strong><code>hidden_dropout_prob</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">attention_probs_dropout_prob</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">DistMeanSelfAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">hidden_dropout_prob</span><span class="p">,</span>
                          <span class="n">attention_probs_dropout_prob</span><span class="p">)</span>

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

<span class="n">mean_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">mean_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mean_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">cov_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">cov_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">cov_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="mf">1e4</span><span class="p">)</span><span class="o">/</span><span class="mf">1e4</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>References</p>
<ol>
<li><a href="https://github.com/sparsh-ai/stanza/blob/S714864/model/attention.py2">https://github.com/sparsh-ai/stanza/blob/S714864/model/attention.py2</a>. <a href="https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers/attention.py">https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers/attention.py</a></li>
</ol>
</blockquote>

</div>
</div>
</div>
</div>
 

